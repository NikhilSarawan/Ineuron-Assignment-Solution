{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce49553",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is the underlying concept of Support Vector Machines?\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "\n",
    "The underlying concept of Support Vector Machines (SVMs) revolves around finding the optimal hyperplane that best\n",
    "separates different classes in a dataset. In the context of classification problems, SVMs aim to find a hyperplane \n",
    "that maximizes the margin, which is the distance between the hyperplane and the nearest data points from each class.\n",
    "This optimal hyperplane effectively acts as a decision boundary, allowing SVMs to classify new, unseen data points \n",
    "into one of the two classes.\n",
    "\n",
    "The term \"support vectors\" refers to the data points that are closest to the decision boundary. These support vectors\n",
    "are crucial because they determine the position and orientation of the hyperplane. SVMs are designed to be effective \n",
    "in high-dimensional spaces, making them suitable for complex classification tasks. SVMs can also handle non-linear \n",
    "decision boundaries through the use of kernel functions, which map the original feature space into a higher-dimensional\n",
    "space, allowing for more complex decision boundaries. SVMs are widely used in both binary and multiclass classification\n",
    "problems, as well as regression tasks. The key idea behind SVMs is to achieve good generalization and robustness by\n",
    "maximizing the margin and effectively separating different classes in the feature space.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. What is the concept of a support vector?\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "In the context of Support Vector Machines (SVMs), support vectors are the data points from the training dataset that\n",
    "are crucial for defining the decision boundary between different classes. These are the data points that lie closest\n",
    "to the hyperplane, which is the optimal decision boundary separating the classes. \n",
    "\n",
    "Support vectors are essential because they determine the position and orientation of the hyperplane. When the SVM \n",
    "algorithm is trained, it identifies these support vectors and uses them to calculate the optimal hyperplane that \n",
    "maximizes the margin between the classes. The margin is the distance between the hyperplane and the nearest support\n",
    "vectors. Maximizing this margin is a key principle of SVMs, as it leads to better generalization and improved\n",
    "performance on unseen data.\n",
    "\n",
    "During the training process, the SVM algorithm focuses on the support vectors, as they are the most challenging \n",
    "data points to classify correctly. The positions of these support vectors are critical for the SVM model's ability\n",
    "to generalize well to new, unseen data. SVMs are named after these support vectors because they \"support\" the \n",
    "construction of the optimal decision boundary. If any of these support vectors were removed or moved, the position\n",
    "of the hyperplane would likely change, impacting the classification performance of the SVM model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. When using SVMs, why is it necessary to scale the inputs?\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "It is necessary to scale the inputs when using Support Vector Machines (SVMs) for several reasons:\n",
    "\n",
    "1. **Sensitivity to Scale:** SVMs are sensitive to the scale of the input features. Features with larger scales might\n",
    "    dominate the optimization process, leading to a biased model. For example, if one feature has values in the range\n",
    "    of 1 to 1000, and another feature has values in the range of 0 to 1, the SVM might give more importance to the \n",
    "    first feature simply due to its larger values.\n",
    "\n",
    "2. **Equal Contribution:** Scaling ensures that all features contribute equally to the distance calculations and \n",
    "    decision boundary. SVM aims to find the optimal hyperplane that best separates classes. If features are not on \n",
    "    the same scale, the decision boundary might be influenced more by features with larger scales, leading to a \n",
    "    suboptimal solution.\n",
    "\n",
    "3. **Faster Convergence:** Scaling the features often helps the optimization algorithm converge faster. \n",
    "    When features are on similar scales, the optimization algorithm can reach the optimal solution more quickly,\n",
    "    saving computational time.\n",
    "\n",
    "4. **Kernel Functions:** If you are using kernel functions (such as radial basis function kernel) in SVM, \n",
    "    scaling becomes even more important. Kernel functions compute the dot product of input samples. If features\n",
    "    are not scaled, the dot products may produce large values for some combinations of features, leading to\n",
    "    numerical instability and incorrect results.\n",
    "\n",
    "By scaling the inputs to a similar range (commonly [0, 1] or [-1, 1]), you ensure that the SVM algorithm can \n",
    "effectively learn the underlying patterns in the data without being biased by the scale of the features. \n",
    "Scaling is a good practice when working with most machine learning algorithms, not just SVMs, to ensure accurate \n",
    "and reliable model training.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4. When an SVM classifier classifies a case, can it output a confidence score? What about a\n",
    "percentage chance?\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "Yes, an SVM classifier can output a confidence score for its predictions, but it does not provide a probability estimate\n",
    "in the same way that some other classifiers (like logistic regression or naive Bayes) do. SVMs make predictions based on \n",
    "the position of a data point relative to the decision boundary (hyperplane). The distance between the data point and the\n",
    "decision boundary can be used as a confidence score. In general, the farther a data point is from the decision boundary,\n",
    "the higher the confidence in the prediction.\n",
    "\n",
    "However, this confidence score does not directly represent a percentage chance or a probability. It doesn't give you the\n",
    "probability of a certain class belonging to a particular data point. SVMs do not naturally output probabilities like some\n",
    "other classifiers do. If you need probability estimates, you can use techniques like Platt scaling or probability\n",
    "calibration methods to convert the SVM decision function scores into probability estimates. These methods involve\n",
    "fitting a logistic regression model to the output scores of the SVM to obtain calibrated probabilities. Keep in mind\n",
    "that these probability estimates might not always be perfectly accurate, especially if the underlying data distribution \n",
    "is complex or the SVM model is not well-calibrated.\n",
    "\n",
    "In summary, SVMs can provide a confidence score based on the distance from the decision boundary, but converting this\n",
    "score into a meaningful probability estimate typically requires additional calibration steps.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "5. Should you train a model on a training set with millions of instances and hundreds of features\n",
    "using the primal or dual form of the SVM problem?\n",
    "\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "\n",
    "When dealing with a large dataset with millions of instances and hundreds of features, it is generally recommended \n",
    "to use the **dual form** of the Support Vector Machine (SVM) problem. The dual form is more computationally efficient\n",
    "in this scenario.\n",
    "\n",
    "In the context of SVM, the primal form involves solving a quadratic optimization problem directly in the feature space, \n",
    "while the dual form involves solving a related optimization problem in a different space (known as the dual space). \n",
    "The dual form of the SVM problem becomes particularly advantageous when the number of instances (samples) is much larger\n",
    "than the number of features.\n",
    "\n",
    "In high-dimensional spaces, the dual form allows the SVM to operate in a space where the number of dimensions is equal \n",
    "to the number of instances, which might be much smaller than the original feature space. This transformation can lead\n",
    "to significant computational savings when dealing with large datasets, making the dual form more practical and efficient\n",
    "for training SVM models on datasets with millions of instances and hundreds of features.\n",
    "\n",
    "Additionally, libraries and implementations of SVM algorithms (such as LIBSVM and scikit-learn in Python) often use \n",
    "optimized solvers that are specifically designed for the dual form, making them well-suited for large-scale datasets.\n",
    "When working with such large datasets, it is advisable to choose SVM implementations that are optimized for efficiency\n",
    "and memory usage to handle the computational challenges effectively.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "6. Let&#39;s say you&#39;ve used an RBF kernel to train an SVM classifier, but it appears to underfit the\n",
    "training collection. Is it better to raise or lower (gamma)? What about the letter C?\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "When an SVM classifier with an RBF (Radial Basis Function) kernel appears to underfit the training data, you can \n",
    "consider adjusting the hyperparameters, specifically the **gamma** parameter and the **C** parameter.\n",
    "\n",
    "1. **Gamma (Î³) Parameter:**\n",
    "   - **Increase Gamma:** A higher gamma value makes the SVM model focus more on individual data points. If your \n",
    "    SVM with RBF kernel is underfitting, increasing gamma can make the model more complex and potentially better\n",
    "    fit the training data. Be cautious, though, as a very high gamma can lead to overfitting, especially if you \n",
    "    have a small dataset.\n",
    "   - **Decrease Gamma:** Lowering the gamma value makes the model generalize more broadly. If your model is overfitting,\n",
    "    reducing gamma can help it generalize better to unseen data. However, if your model is underfitting, decreasing \n",
    "    gamma might make the underfitting issue worse.\n",
    "\n",
    "2. **C Parameter:**\n",
    "   - **Increase C:** The C parameter is the regularization parameter in SVM. It controls the trade-off between having \n",
    "    a smooth decision boundary and classifying the training points correctly. Increasing C allows the model to fit the \n",
    "    training data more accurately, potentially reducing underfitting. However, a very high C can lead to overfitting.\n",
    "   - **Decrease C:** Lowering the C parameter increases the regularization strength, making the decision boundary smoother.\n",
    "    Smaller values of C encourage a simpler decision boundary, which can help prevent overfitting. If your model is \n",
    "    underfitting, decreasing C might help by encouraging a smoother decision boundary. However, if C is already low,\n",
    "    reducing it further may not be beneficial.\n",
    "\n",
    "It's essential to perform hyperparameter tuning systematically, for example, by using techniques like grid search or \n",
    "random search coupled with cross-validation. This allows you to evaluate different combinations of gamma and C values\n",
    "to find the one that performs best on your dataset. Always use a separate validation dataset to assess the model's \n",
    "performance during the tuning process to avoid overfitting the hyperparameters to the training data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should\n",
    "the QP parameters (H, f, A, and b) be set?\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "In the context of solving the soft margin linear SVM problem using a Quadratic Programming (QP) solver,\n",
    "the problem can be formulated as follows:\n",
    "\n",
    "**Objective Function:**\n",
    "Minimize:\n",
    "\\[ \\frac{1}{2} \\mathbf{w}^T \\mathbf{w} + C \\sum_{i=1}^{m} \\xi_i \\]\n",
    "\n",
    "**Constraints:**\n",
    "Subject to:\n",
    "\\[ y^{(i)}(\\mathbf{w}^T \\mathbf{x}^{(i)} + b) \\geq 1 - \\xi_i \\]\n",
    "\\[ \\xi_i \\geq 0 \\]\n",
    "for \\(i = 1, 2, ..., m\\)\n",
    "\n",
    "Where:\n",
    "- \\(\\mathbf{w}\\) is the weight vector.\n",
    "- \\(b\\) is the bias term.\n",
    "- \\(\\xi_i\\) are slack variables.\n",
    "- \\(C\\) is the regularization parameter (penalty for misclassification).\n",
    "- \\(\\mathbf{x}^{(i)}\\) is the \\(i\\)th training instance.\n",
    "- \\(y^{(i)}\\) is the class label for the \\(i\\)th training instance.\n",
    "\n",
    "To set up the QP parameters (\\(H\\), \\(f\\), \\(A\\), and \\(b\\)) for the solver, you need to convert the soft \n",
    "margin SVM problem into the standard QP problem format, which is of the form:\n",
    "\n",
    "\\[ \\text{Minimize } \\frac{1}{2} \\mathbf{x}^T H \\mathbf{x} + \\mathbf{f}^T \\mathbf{x} \\]\n",
    "Subject to:\n",
    "\\[ A\\mathbf{x} = \\mathbf{b} \\]\n",
    "\\[ \\mathbf{g} \\leq \\mathbf{x} \\leq \\mathbf{h} \\]\n",
    "\n",
    "In the SVM problem:\n",
    "\n",
    "- \\(H\\) is the Hessian matrix, which is a matrix of second derivatives of the objective function.\n",
    "- \\(f\\) is the linear coefficient vector in the objective function.\n",
    "- \\(A\\) is the matrix for the equality constraints (\\(A\\mathbf{x} = \\mathbf{b}\\)).\n",
    "- \\(b\\) is the vector for the equality constraints.\n",
    "- \\(g\\) and \\(h\\) represent the lower and upper bounds for the variables \\(\\mathbf{x}\\).\n",
    "\n",
    "To set these parameters, you need to perform specific transformations and computations based on the SVM \n",
    "problem formulation. It's highly recommended to use SVM libraries and software packages (such as LIBSVM,\n",
    "scikit-learn in Python, or SVMlight) that handle these computations internally. These libraries are optimized \n",
    "and can efficiently solve SVM problems, including soft margin SVM, without the need for manual setup of QP parameters.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and\n",
    "an SGDClassifier. See if you can get them to make a model that is similar to yours.\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "\n",
    "Certainly! In this scenario, you can train three different classifiers on a linearly separable dataset and compare\n",
    "their performance. Here's how you can do it using scikit-learn in Python:\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a linearly separable dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train LinearSVC\n",
    "linear_svc = LinearSVC()\n",
    "linear_svc.fit(X_train, y_train)\n",
    "linear_svc_accuracy = accuracy_score(y_test, linear_svc.predict(X_test))\n",
    "print(\"LinearSVC Accuracy:\", linear_svc_accuracy)\n",
    "\n",
    "# Train SVC with linear kernel\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train, y_train)\n",
    "svc_accuracy = accuracy_score(y_test, svc.predict(X_test))\n",
    "print(\"SVC Accuracy:\", svc_accuracy)\n",
    "\n",
    "# Train SGDClassifier with linear loss (equivalent to linear SVM)\n",
    "sgd_classifier = SGDClassifier(loss='hinge', alpha=0.01, max_iter=1000, random_state=42)\n",
    "sgd_classifier.fit(X_train, y_train)\n",
    "sgd_accuracy = accuracy_score(y_test, sgd_classifier.predict(X_test))\n",
    "print(\"SGDClassifier Accuracy:\", sgd_accuracy)\n",
    "```\n",
    "\n",
    "In this code, we first create a linearly separable dataset using the `make_classification` function from scikit-learn. \n",
    "We then split the dataset into training and testing sets. We train three different classifiers: `LinearSVC`, \n",
    "    `SVC` with a linear kernel, and `SGDClassifier` with a linear loss function (equivalent to linear SVM).\n",
    "    Finally, we calculate and print the accuracy of each model on the test data.\n",
    "\n",
    "Please note that the accuracy scores might vary based on the random seed and the specific dataset generated.\n",
    "The key is to observe that `LinearSVC`, `SVC` with a linear kernel, and `SGDClassifier` should provide similar\n",
    "accuracy scores since they are all trained on a linearly separable dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "9. On the MNIST dataset, train an SVM classifier. You&#39;ll need to use one-versus-the-rest to assign all\n",
    "\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "Certainly! Training a Support Vector Machine (SVM) classifier on the MNIST dataset involves using the one-versus-the-rest\n",
    "(OvR) strategy, where you train a separate binary classifier for each digit (0 to 9) and classify an image based on the\n",
    "decision scores from these individual classifiers. Here's an example of how you can do this using scikit-learn in Python:\n",
    "\n",
    "```python\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the MNIST dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Split the data into features (X) and labels (y)\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train 10 binary SVM classifiers (one for each digit)\n",
    "svm_classifiers = []\n",
    "for digit in range(10):\n",
    "    # Create a binary target variable for the current digit\n",
    "    binary_labels = (y_train == digit).astype(int)\n",
    "    # Train a binary SVM classifier for the current digit\n",
    "    svm_classifier = SVC(kernel='linear', C=1.0)\n",
    "    svm_classifier.fit(X_train, binary_labels)\n",
    "    svm_classifiers.append(svm_classifier)\n",
    "\n",
    "# Predict using all 10 classifiers and select the one with the highest decision score\n",
    "predictions = []\n",
    "for classifier in svm_classifiers:\n",
    "    decision_scores = classifier.decision_function(X_test)\n",
    "    predictions.append(decision_scores)\n",
    "\n",
    "# Select the digit corresponding to the classifier with the highest decision score\n",
    "predicted_labels = [max(range(10), key=lambda x: pred[x]) for pred in zip(*predictions)]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "print(\"Accuracy on the MNIST test set:\", accuracy)\n",
    "```\n",
    "\n",
    "In this example, we first load the MNIST dataset and split it into training and testing sets. We then train 10 binary\n",
    "SVM classifiers using the OvR strategy. Each classifier is trained to distinguish one specific digit from the rest. \n",
    "During prediction, decision scores from all 10 classifiers are obtained, and the digit with the highest decision score\n",
    "is predicted.\n",
    "\n",
    "Make sure to adjust hyperparameters like the `C` parameter and kernel type according to your specific use case and\n",
    "perform further tuning if necessary to achieve the best results.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want\n",
    "to tune the hyperparameters using small validation sets. What level of precision can you achieve?\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "\n",
    "The accuracy of an SVM classifier on the MNIST dataset can vary based on several factors, including the choice of\n",
    "hyperparameters, the type of kernel used, and the quality of features. SVM classifiers are powerful, but finding \n",
    "the optimal set of hyperparameters can significantly impact their performance.\n",
    "\n",
    "To tune the hyperparameters efficiently, you can use techniques like grid search or random search with cross-validation\n",
    "on a small validation set. Here's an example of how you can perform grid search with cross-validation to find the best\n",
    "hyperparameters for an SVM classifier using scikit-learn:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {'C': [0.1, 1, 10],\n",
    "              'kernel': ['linear', 'rbf']}\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(svm_classifier, param_grid, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train the SVM classifier with the best hyperparameters\n",
    "best_svm_classifier = SVC(C=best_params['C'], kernel=best_params['kernel'])\n",
    "best_svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = best_svm_classifier.score(X_test, y_test)\n",
    "print(\"Accuracy on the MNIST test set with tuned hyperparameters:\", accuracy)\n",
    "```\n",
    "\n",
    "In this example, `param_grid` defines the hyperparameters and their possible values. Grid search with cross-validation\n",
    "(`cv=3` means 3-fold cross-validation) is performed to find the best combination of hyperparameters. The best\n",
    "hyperparameters are then used to train the final SVM classifier, and its accuracy is evaluated on the test set.\n",
    "\n",
    "The level of precision you can achieve will depend on the chosen hyperparameters, the quality of features, and \n",
    "the amount of training data. By using grid search and cross-validation, you can systematically search through \n",
    "different hyperparameter combinations and select the ones that result in the highest accuracy on the validation data, \n",
    "leading to a more precise model. Keep in mind that achieving high accuracy on the MNIST dataset with SVMs is \n",
    "definitely possible, and precise results can be obtained with proper hyperparameter tuning and feature preprocessing.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "11. On the California housing dataset, train an SVM regressor.\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "Certainly! Training a Support Vector Machine (SVM) regressor on the California housing dataset involves predicting\n",
    "a continuous target variable (median house value) based on the input features. Here's an example of how you can do\n",
    "this using scikit-learn in Python:\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the California housing dataset\n",
    "california_housing = fetch_california_housing()\n",
    "X, y = california_housing.data, california_housing.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train an SVM regressor\n",
    "svm_regressor = SVR(kernel='linear', C=1.0, epsilon=0.2)\n",
    "svm_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = svm_regressor.predict(X_test)\n",
    "\n",
    "# Calculate mean squared error (MSE) to evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "```\n",
    "\n",
    "In this example, we first load the California housing dataset and split it into training and testing sets.\n",
    "The input features are standardized using `StandardScaler` to ensure all features are on a similar scale. Then, \n",
    "we train an SVM regressor (`SVR`) with a linear kernel. You can adjust hyperparameters such as `C` (regularization parameter)\n",
    "and the choice of kernel according to your specific use case.\n",
    "\n",
    "After training the regressor, predictions are made on the test set, and the mean squared error (MSE) is calculated\n",
    "to evaluate the model's performance. The lower the MSE, the better the model fits the data. You can experiment with\n",
    "different kernels and hyperparameters to optimize the SVM regressor for the California housing dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
