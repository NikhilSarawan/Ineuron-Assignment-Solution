{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad2c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Define the Bayesian interpretation of probability.\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "The Bayesian interpretation of probability is a mathematical framework for understanding uncertainty and making \n",
    "probabilistic inferences about events or hypotheses. Unlike the frequentist interpretation, which considers probability\n",
    "as the long-term frequency of an event occurring over multiple trials, Bayesian probability represents a degree of\n",
    "belief or confidence in the occurrence of an event based on available evidence.\n",
    "\n",
    "In Bayesian probability theory, prior beliefs about the probability of an event are updated in the light of new evidence\n",
    "to obtain a posterior probability. Bayes' theorem is the fundamental theorem behind this interpretation and is expressed \n",
    "as follows:\n",
    "\n",
    "\\[ P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)} \\]\n",
    "\n",
    "Where:\n",
    "- \\( P(A|B) \\) is the posterior probability of event A given evidence B.\n",
    "- \\( P(B|A) \\) is the likelihood of observing evidence B given that A is true.\n",
    "- \\( P(A) \\) is the prior probability of event A before considering the evidence.\n",
    "- \\( P(B) \\) is the probability of observing evidence B.\n",
    "\n",
    "2. **Probability of the Union of Two Events:**\n",
    "The probability of the union of two events A and B (denoted as \\( P(A \\cup B) \\)) can be calculated using the following\n",
    "equation:\n",
    "\n",
    "\\[ P(A \\cup B) = P(A) + P(B) - P(A \\cap B) \\]\n",
    "\n",
    "Where \\( P(A) \\) represents the probability of event A, \\( P(B) \\) represents the probability of event B, and \n",
    "\\( P(A \\cap B) \\) represents the probability of both events A and B occurring simultaneously (joint probability).\n",
    "\n",
    "3. **Joint Probability:**\n",
    "Joint probability refers to the probability of two or more events occurring simultaneously. For two events A and B,\n",
    "the joint probability is denoted as \\( P(A \\cap B) \\) and represents the likelihood of both events A and B occurring\n",
    "together. The formula for joint probability depends on the specific context and relationships between the events.\n",
    "\n",
    "4. **Chain Rule of Probability:**\n",
    "The chain rule of probability allows the calculation of the joint probability of multiple events by breaking it down \n",
    "into conditional probabilities. For example, for three events A, B, and C, the chain rule states:\n",
    "\n",
    "\\[ P(A \\cap B \\cap C) = P(A) \\times P(B|A) \\times P(C|A \\cap B) \\]\n",
    "\n",
    "This formula generalizes to more than three events as well.\n",
    "\n",
    "5. **Conditional Probability:**\n",
    "Conditional probability represents the probability of an event occurring given that another event has already occurred.\n",
    "It is denoted as \\( P(A|B) \\) and can be calculated using the formula:\n",
    "\n",
    "\\[ P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\]\n",
    "\n",
    "Where \\( P(A|B) \\) is the conditional probability of event A given event B.\n",
    "\n",
    "6. **Continuous Random Variables:**\n",
    "Continuous random variables can take any real value within a certain range. Unlike discrete random variables, \n",
    "which can only take distinct, separate values, continuous random variables can have an infinite number of possible\n",
    "values within a given interval. Examples include measurements such as height, weight, or time.\n",
    "\n",
    "7. **Bernoulli Distribution:**\n",
    "The Bernoulli distribution models a single experiment with two possible outcomes: success (usually denoted as 1)\n",
    "    and failure (usually denoted as 0). The probability mass function of a Bernoulli random variable with parameter\n",
    "    \\( p \\) is given by:\n",
    "\n",
    "\\[ P(X = k) = p^k \\times (1 - p)^{1 - k} \\]\n",
    "\n",
    "Where \\( k \\) can be 0 or 1, and \\( p \\) is the probability of success.\n",
    "\n",
    "8. **Binomial Distribution:**\n",
    "The binomial distribution describes the number of successes in a fixed number of independent Bernoulli trials. \n",
    "If \\( X \\) follows a binomial distribution with parameters \\( n \\) (number of trials) and \\( p \\)\n",
    "(probability of success in each trial), the probability mass function is:\n",
    "\n",
    "\\[ P(X = k) = \\binom{n}{k} \\times p^k \\times (1 - p)^{n - k} \\]\n",
    "\n",
    "Where \\( \\binom{n}{k} \\) represents the binomial coefficient, equal to \\( \\frac{n!}{k!(n-k)!} \\).\n",
    "\n",
    "9. **Poisson Distribution:**\n",
    "The Poisson distribution models the number of events that occur within a fixed interval of time or space.\n",
    "If \\( X \\) follows a Poisson distribution with parameter \\( \\lambda \\) (average rate of events), \n",
    "the probability mass function is:\n",
    "\n",
    "\\[ P(X = k) = \\frac{e^{-\\lambda} \\times \\lambda^k}{k!} \\]\n",
    "\n",
    "Where \\( e \\) is the base of the natural logarithm.\n",
    "\n",
    "10. **Covariance:**\n",
    "Covariance measures the degree to which two random variables change together. It is a statistical measure of the \n",
    "relationship between the deviations of two variables from their respective means. Positive covariance indicates a \n",
    "positive relationship, negative covariance indicates a negative relationship, and covariance close to zero suggests \n",
    "little to no linear relationship.\n",
    "\n",
    "11. **Correlation:**\n",
    "Correlation is a standardized measure of the strength and direction of the linear relationship between two variables.\n",
    "Unlike covariance, correlation values range from -1 to 1. A correlation of 1 indicates a perfect positive linear relationship, \n",
    "-1 indicates a perfect negative linear relationship, and 0 indicates no linear relationship.\n",
    "\n",
    "12. **Sampling With Replacement:**\n",
    "Sampling with replacement means that each item drawn from a population is returned to the population before the\n",
    "next item is drawn. For example, if you have a bag of marbles and you draw one marble, note its color,\n",
    "put it back into the bag, and then draw another marble, you are sampling with replacement.\n",
    "\n",
    "13. **Sampling Without Replacement:**\n",
    "Sampling without replacement means that each item drawn from a population is not returned to the population \n",
    "before the next item is drawn. Using the same example of marbles, if you draw one marble and do not put it back\n",
    "into the bag before drawing the next marble, you are sampling without replacement.\n",
    "\n",
    "14. **Hypothesis:**\n",
    "A hypothesis is a specific, testable statement or prediction about a phenomenon or relationship between variables.\n",
    "In the context of scientific research, a hypothesis is formulated to be tested through experiments or observations\n",
    "to determine its validity. For example, a hypothesis could be: \"Increasing the amount of sunlight will lead to \n",
    "    plant growth.\"\n",
    "\n",
    "Please note that the examples provided in the explanations are for illustrative purposes and may not reflect real-world\n",
    "situations or values.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. Define probability of a union of two events with equation.\n",
    "\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "\n",
    "\n",
    "The probability of the union of two events A and B (denoted as \\( P(A \\cup B) \\)) represents the probability that either\n",
    "event A or event B or both occur. This probability can be calculated using the following formula, known as the addition\n",
    "rule for probability:\n",
    "\n",
    "\\[ P(A \\cup B) = P(A) + P(B) - P(A \\cap B) \\]\n",
    "\n",
    "Where:\n",
    "- \\( P(A) \\) represents the probability of event A occurring.\n",
    "- \\( P(B) \\) represents the probability of event B occurring.\n",
    "- \\( P(A \\cap B) \\) represents the probability of both events A and B occurring simultaneously.\n",
    "\n",
    "The subtraction term (\\( P(A \\cap B) \\)) is included in the formula to avoid double-counting the cases where both events\n",
    "A and B occur. This term represents the probability of the intersection of events A and B, i.e., the probability that\n",
    "both events A and B happen at the same time. By subtracting this overlap, the formula ensures that the cases where\n",
    "A and B both occur are counted only once in the total probability of the union of the two events.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. What is joint probability? What is its formula?\n",
    "\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "\n",
    "Joint probability refers to the probability of two or more events occurring simultaneously. For example, if we have\n",
    "two events A and B, the joint probability \\( P(A \\cap B) \\) represents the likelihood that both events A and B happen\n",
    "at the same time.\n",
    "\n",
    "The formula for joint probability depends on the specific relationship between the events. For two independent events\n",
    "A and B, where the occurrence of one event does not affect the occurrence of the other, the joint probability is \n",
    "calculated as the product of their individual probabilities:\n",
    "\n",
    "\\[ P(A \\cap B) = P(A) \\times P(B) \\]\n",
    "\n",
    "In the case of dependent events, where the occurrence of one event does affect the occurrence of the other,\n",
    "the joint probability is calculated using the conditional probability of one event given the other event:\n",
    "\n",
    "\\[ P(A \\cap B) = P(A) \\times P(B|A) \\]\n",
    "\n",
    "This formula states that the joint probability of events A and B is equal to the probability of A multiplied by the\n",
    "conditional probability of B given A. Similarly, you can calculate \\( P(B \\cap A) \\) using \\( P(B) \\times P(A|B) \\).\n",
    "Note that in most cases, \\( P(A \\cap B) \\) is equal to \\( P(B \\cap A) \\), following the commutative property of\n",
    "intersection in set theory.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4. What is chain rule of probability?\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "The chain rule of probability is a fundamental concept in probability theory that allows the calculation of the joint\n",
    "probability of multiple events by breaking it down into conditional probabilities. It is derived from the definition\n",
    "of conditional probability and is particularly useful when dealing with complex scenarios involving multiple events.\n",
    "\n",
    "For a sequence of events \\( A_1, A_2, \\ldots, A_n \\), the chain rule states that the joint probability of all these \n",
    "events occurring can be expressed as the product of conditional probabilities:\n",
    "\n",
    "\\[ P(A_1 \\cap A_2 \\cap \\ldots \\cap A_n) = P(A_1) \\times P(A_2|A_1) \\times P(A_3|A_1 \\cap A_2) \\times \\ldots \\times\n",
    "  P(A_n|A_1 \\cap A_2 \\cap \\ldots \\cap A_{n-1}) \\]\n",
    "\n",
    "In words, the probability of all events \\( A_1, A_2, \\ldots, A_n \\) occurring together is equal to the probability\n",
    "of the first event \\( A_1 \\) multiplied by the probability of the second event \\( A_2 \\) given that \\( A_1 \\) has occurred,\n",
    "multiplied by the probability of the third event \\( A_3 \\) given that \\( A_1 \\) and \\( A_2 \\) have occurred, and so on,\n",
    "up to the last event \\( A_n \\) given that all preceding events \\( A_1, A_2, \\ldots, A_{n-1} \\) have occurred.\n",
    "\n",
    "The chain rule is a powerful tool for calculating joint probabilities in more complex situations and is essential in \n",
    "various fields such as statistics, machine learning, and data science, where understanding and computing probabilities\n",
    "of multiple events are common tasks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "5. What is conditional probability means? What is the formula of it?\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "Conditional probability is the probability of an event occurring given that another event has already occurred.\n",
    "It quantifies the likelihood of an event under a specific condition. In other words, conditional probability is \n",
    "the probability of event A happening, given that event B has already occurred. It is denoted as \\( P(A|B) \\),\n",
    "read as \"the probability of A given B.\"\n",
    "\n",
    "The formula for conditional probability is given by:\n",
    "\n",
    "\\[ P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\]\n",
    "\n",
    "Where:\n",
    "- \\( P(A|B) \\) represents the conditional probability of event A given event B.\n",
    "- \\( P(A \\cap B) \\) represents the joint probability of both events A and B occurring simultaneously.\n",
    "- \\( P(B) \\) represents the probability of event B occurring.\n",
    "\n",
    "In words, the conditional probability of A given B is equal to the joint probability of A and B divided by the\n",
    "probability of B. This formula essentially adjusts the probability of event A, taking into account the fact that \n",
    "event B has already occurred. It provides a way to update our beliefs about event A based on the information that event\n",
    "B has happened.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "6. What are continuous random variables?\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "\n",
    "Continuous random variables are variables that can take any real value within a specific range or interval.\n",
    "Unlike discrete random variables, which can only assume distinct, separate values (such as integers),\n",
    "continuous random variables have an infinite number of possible values within a given interval.\n",
    "\n",
    "Examples of continuous random variables include:\n",
    "\n",
    "1. **Height:** Height can take on any real value within a certain range, such as between 4.5 feet and 6.7 feet.\n",
    "2. **Weight:** Weight can be any positive real number, measured in pounds or kilograms.\n",
    "3. **Temperature:** Temperature can be measured as a continuous variable in degrees Celsius or Fahrenheit.\n",
    "4. **Time:** Time can be a continuous variable when measured with high precision, such as hours, minutes, seconds,\n",
    "    or even fractions of a second.\n",
    "5. **Distance:** Distance can take any non-negative real value, such as meters, kilometers, or miles.\n",
    "\n",
    "In the context of continuous random variables, probability is associated with intervals rather than specific values.\n",
    "The probability density function (PDF) describes the likelihood of the random variable falling within a particular interval.\n",
    "Unlike the probability mass function (PMF) used for discrete random variables, the area under the PDF curve within \n",
    "a given interval represents the probability of the random variable falling within that interval.\n",
    "\n",
    "Continuous random variables are fundamental in various fields, especially in statistics, physics, engineering,\n",
    "and finance, where precise measurements and modeling of real-world phenomena are essential.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "7. What are Bernoulli distributions? What is the formula of it?\n",
    "\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "\n",
    "A Bernoulli distribution is a discrete probability distribution that models a single experiment with two possible\n",
    "outcomes: success and failure. It is named after Swiss mathematician Jacob Bernoulli. In the context of a Bernoulli\n",
    "    distribution, an event is called a \"success\" if it happens and a \"failure\" if it does not.\n",
    "\n",
    "The probability mass function (PMF) of a Bernoulli random variable is defined as follows:\n",
    "\n",
    "\\[ P(X = k) = \\begin{cases} \n",
    "p & \\text{if } k = 1 \\\\\n",
    "1 - p & \\text{if } k = 0 \n",
    "\\end{cases} \\]\n",
    "\n",
    "Where:\n",
    "- \\( P(X = k) \\) is the probability of the random variable X taking on the value k (either 0 or 1).\n",
    "- \\( p \\) is the probability of success (i.e., the probability that the random variable equals 1).\n",
    "- \\( 1 - p \\) is the probability of failure (i.e., the probability that the random variable equals 0).\n",
    "\n",
    "The Bernoulli distribution is a special case of the binomial distribution where the number of trials (n) is 1. \n",
    "In other words, if you perform a sequence of independent experiments, each with a binary outcome, and you are \n",
    "interested in the probability of success in a single experiment, you can model it using a Bernoulli distribution.\n",
    "\n",
    "The expected value (mean) of a Bernoulli random variable is \\( \\mu = p \\), and its variance is \\( \\sigma^2 = p(1 - p) \\).\n",
    "The Bernoulli distribution is widely used in various fields, including statistics, machine learning, and decision-making\n",
    "processes where binary outcomes are encountered.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "8. What is binomial distribution? What is the formula?\n",
    "\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "The binomial distribution is a discrete probability distribution that describes the number of successes in a fixed \n",
    "number of independent Bernoulli trials, where each trial results in a binary outcome (success or failure).\n",
    "The trials are independent, meaning the outcome of one trial does not affect the outcome of the other trials. \n",
    "The binomial distribution is widely used in statistics, biology, psychology, and various other fields to model\n",
    "situations involving multiple binary events.\n",
    "\n",
    "In the binomial distribution, if \\( X \\) is the number of successes in \\( n \\) trials, where each trial has a\n",
    "probability \\( p \\) of success and \\( q = 1 - p \\) of failure, the probability mass function (PMF) of the binomial \n",
    "distribution is given by:\n",
    "\n",
    "\\[ P(X = k) = \\binom{n}{k} \\times p^k \\times q^{(n - k)} \\]\n",
    "\n",
    "Where:\n",
    "- \\( P(X = k) \\) is the probability of getting exactly \\( k \\) successes in \\( n \\) trials.\n",
    "- \\( \\binom{n}{k} \\) represents the binomial coefficient, equal to \\( \\frac{n!}{k!(n - k)!} \\), which calculates\n",
    "the number of ways to choose \\( k \\) successes from \\( n \\) trials.\n",
    "- \\( p \\) is the probability of success in a single trial.\n",
    "- \\( q = 1 - p \\) is the probability of failure in a single trial.\n",
    "- \\( k \\) can range from 0 to \\( n \\), inclusive.\n",
    "\n",
    "The mean (expected value) of a binomial distribution is \\( \\mu = np \\), and the variance is \\( \\sigma^2 = npq \\).\n",
    "\n",
    "The binomial distribution is applicable when there are two possible outcomes (success and failure) and the trials\n",
    "are independent and identically distributed. It is often used to model scenarios such as the number of heads \n",
    "obtained when flipping a coin multiple times or the number of defective items in a sample from a production batch.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "9. What is Poisson distribution? What is the formula?\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "\n",
    "The Poisson distribution is a discrete probability distribution that expresses the probability of a given number of\n",
    "events occurring in a fixed interval of time or space. It is often used to model rare events or events that occur \n",
    "independently at a constant mean rate over time or space. The Poisson distribution is named after the French\n",
    "mathematician Sim√©on Denis Poisson.\n",
    "\n",
    "If \\( X \\) is a random variable representing the number of events occurring in a fixed interval, and \\( \\lambda \\) is\n",
    "the average number of events in that interval, the probability mass function (PMF) of the Poisson distribution is given by:\n",
    "\n",
    "\\[ P(X = k) = \\frac{e^{-\\lambda} \\times \\lambda^k}{k!} \\]\n",
    "\n",
    "Where:\n",
    "- \\( P(X = k) \\) is the probability of observing \\( k \\) events in the given interval.\n",
    "- \\( e \\) is the base of the natural logarithm, approximately equal to 2.71828.\n",
    "- \\( \\lambda \\) (lambda) is the average number of events that occur in the interval.\n",
    "- \\( k \\) is the actual number of events observed in the interval.\n",
    "- \\( k! \\) represents the factorial of \\( k \\), which is the product of all positive integers from 1 to \\( k \\).\n",
    "\n",
    "In the Poisson distribution, both the mean (\\( \\mu \\)) and the variance (\\( \\sigma^2 \\)) are equal to \\( \\lambda \\).\n",
    "\n",
    "The Poisson distribution is appropriate for events that are rare, infrequent, and independent of each other.\n",
    "Common applications include modeling the number of phone calls at a call center within a specific time period, \n",
    "the number of accidents at a busy intersection in a day, or the number of emails received per hour.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "10. Define covariance.\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "\n",
    "Covariance is a statistical measure that quantifies the degree to which two random variables change together. \n",
    "In other words, it measures how changes in one variable are associated with changes in another variable. \n",
    "\n",
    "A positive covariance indicates that the two variables tend to increase or decrease together, while a negative\n",
    "covariance indicates that one variable tends to increase when the other decreases. A covariance close to zero \n",
    "suggests little to no linear relationship between the variables, but it does not imply independence. Two variables\n",
    "can be uncorrelated (covariance = 0) and still be related in a nonlinear manner.\n",
    "\n",
    "The formula for covariance between two random variables X and Y, each with n data points, is given by:\n",
    "\n",
    "\\[ \\text{Cov}(X, Y) = \\frac{\\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})}{n} \\]\n",
    "\n",
    "Where:\n",
    "- \\( X_i \\) and \\( Y_i \\) are the individual data points of variables X and Y, respectively.\n",
    "- \\( \\bar{X} \\) and \\( \\bar{Y} \\) are the means (averages) of variables X and Y, respectively.\n",
    "- \\( n \\) is the number of data points.\n",
    "\n",
    "It's important to note that covariance is not standardized and can take on any real value. Therefore,\n",
    "it can be challenging to interpret the magnitude of covariance. For this reason, correlation, which is\n",
    "a standardized version of covariance, is often used to measure the strength and direction of the linear \n",
    "relationship between two variables.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "11. Define correlation\n",
    "\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "Correlation is a statistical measure that quantifies the strength and direction of the linear relationship between \n",
    "two random variables. Unlike covariance, correlation is a standardized measure, meaning it is scaled to always fall\n",
    "between -1 and 1. A correlation of +1 indicates a perfect positive linear relationship, a correlation of -1 indicates \n",
    "a perfect negative linear relationship, and a correlation of 0 indicates no linear relationship between the variables.\n",
    "\n",
    "The most commonly used measure of correlation is Pearson correlation coefficient, denoted as \\( r \\). For two random \n",
    "variables X and Y, each with n data points, the formula for Pearson correlation coefficient is given by:\n",
    "\n",
    "\\[ r = \\frac{\\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i=1}^{n} (X_i - \\bar{X})^2 \\times \\sum_{i=1}^{n} \n",
    "                                                                  (Y_i - \\bar{Y})^2}} \\]\n",
    "\n",
    "Where:\n",
    "- \\( X_i \\) and \\( Y_i \\) are the individual data points of variables X and Y, respectively.\n",
    "- \\( \\bar{X} \\) and \\( \\bar{Y} \\) are the means (averages) of variables X and Y, respectively.\n",
    "- \\( n \\) is the number of data points.\n",
    "\n",
    "The correlation coefficient \\( r \\) ranges from -1 to 1. A positive \\( r \\) value indicates a positive correlation\n",
    "(as one variable increases, the other variable tends to increase), while a negative \\( r \\) value indicates a negative\n",
    "correlation (as one variable increases, the other variable tends to decrease). A correlation coefficient close to 0 \n",
    "suggests little to no linear relationship between the variables.\n",
    "\n",
    "Correlation does not imply causation; even if two variables are correlated, it does not mean that changes in one \n",
    "variable cause changes in the other. Correlation measures the strength and direction of the linear relationship, \n",
    "but it cannot capture complex relationships or infer causality.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "12. Define sampling with replacement. Give example.\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "\n",
    "Sampling with replacement is a method of selecting items from a population in which each item is chosen randomly, \n",
    "and after each selection, the chosen item is put back into the population. This means that the same item can be \n",
    "selected more than once in the process.\n",
    "\n",
    "Here's an example to illustrate sampling with replacement:\n",
    "\n",
    "Imagine you have a bag containing five colored marbles: two red marbles, one blue marble, one green marble, \n",
    "    and one yellow marble. If you want to randomly select two marbles from the bag with replacement, \n",
    "    the process might look like this:\n",
    "\n",
    "1. **First Draw:** You reach into the bag and randomly select a marble. Let's say you pick a red marble. \n",
    "    After recording the color, you put the red marble back into the bag.\n",
    "   \n",
    "   - Bag after the first draw: [2 red, 1 blue, 1 green, 1 yellow]\n",
    "\n",
    "2. **Second Draw:** You reach into the bag again and randomly select another marble. This time you might pick\n",
    "    the green marble. You record the color and put the green marble back into the bag.\n",
    "\n",
    "   - Bag after the second draw: [2 red, 1 blue, 1 green, 1 yellow]\n",
    "\n",
    "In this example, since you're sampling with replacement, the same marbles are always available for selection in\n",
    "each draw. Therefore, the probabilities remain constant across multiple draws. Sampling with replacement is commonly \n",
    "used in situations where the population is large, and the effect of removing an individual item is negligible compared\n",
    "to the size of the population.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "13. What is sampling without replacement? Give example.\n",
    "\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "Sampling without replacement is a method of selecting items from a population in which each item is chosen randomly and, \n",
    "once selected, is not returned to the population. This means that each item can only be chosen once, and the population \n",
    "size decreases with each selection.\n",
    "\n",
    "Here's an example to illustrate sampling without replacement:\n",
    "\n",
    "Imagine you have a deck of 52 playing cards. If you want to randomly select two cards from the deck without replacement, \n",
    "the process might look like this:\n",
    "\n",
    "1. **First Draw:** You randomly select a card from the deck. Let's say you pick the Ace of Spades. After recording the card, \n",
    "    you do not put it back into the deck.\n",
    "\n",
    "   - Remaining cards: 51\n",
    "\n",
    "2. **Second Draw:** You reach into the remaining 51 cards and randomly select another card. This time you might pick the\n",
    "    Seven of Hearts.\n",
    "\n",
    "   - Remaining cards: 50\n",
    "\n",
    "In this example, since you're sampling without replacement, the composition of the deck changes after each draw, \n",
    "affecting the probabilities of subsequent selections. After the first draw, there are 51 cards left in the deck, \n",
    "so the probability of drawing a specific card on the second draw is different from the first draw.\n",
    "\n",
    "Sampling without replacement is often used when you want to ensure that each element in the population is selected \n",
    "exactly once and prevents duplicate selections in statistical studies and experiments.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "14. What is hypothesis? Give example.\n",
    "\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "\n",
    "In the context of statistics and scientific research, a hypothesis is a specific, testable statement or prediction \n",
    "about a phenomenon or relationship between variables. It is a clear and concise statement that suggests an explanation\n",
    "for an observed phenomenon or predicts the outcome of a future experiment or observation. Hypotheses are essential in\n",
    "the scientific method as they provide a basis for making predictions and conducting experiments to test those predictions.\n",
    "\n",
    "There are two main types of hypotheses:\n",
    "\n",
    "1. **Null Hypothesis (H0):** The null hypothesis is a statement that there is no significant difference or relationship\n",
    "    between variables. It often represents the status quo or a default assumption that researchers aim to challenge or refute.\n",
    "    The null hypothesis is denoted as H0.\n",
    "\n",
    "   Example of a null hypothesis: \"There is no significant difference in test scores between students who receive tutoring \n",
    "    and those who do not.\"\n",
    "\n",
    "2. **Alternative Hypothesis (Ha or H1):** The alternative hypothesis is a statement that contradicts the null hypothesis\n",
    "    and suggests that there is a significant difference or relationship between variables. The alternative hypothesis is\n",
    "    what researchers are trying to provide evidence for. It is denoted as Ha or sometimes as H1.\n",
    "\n",
    "   Example of an alternative hypothesis: \"Students who receive tutoring have significantly higher test scores than those \n",
    "    who do not.\"\n",
    "\n",
    "In hypothesis testing, researchers collect data and perform statistical analyses to determine whether the evidence \n",
    "supports the null hypothesis or provides enough evidence to reject it in favor of the alternative hypothesis. \n",
    "The outcome of the hypothesis test helps researchers draw conclusions about the phenomenon they are investigating.\n",
    "\n",
    "It's important to note that hypotheses are not absolute truths but rather statements that are subject to testing\n",
    "and revision based on empirical evidence and further research.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
