{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d73e459",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is your definition of clustering? What are a few clustering algorithms you might think of?\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "Clustering is a type of unsupervised machine learning technique that involves grouping similar data points together\n",
    "into clusters or segments based on their similarities. The goal of clustering is to identify patterns and structures\n",
    "within the data without any prior knowledge of the groupings. In other words, it aims to find natural groupings in \n",
    "the data such that data points within the same cluster are more similar to each other than those in different clusters.\n",
    "\n",
    "There are several clustering algorithms, each with its own approach to defining and identifying clusters. Here are a\n",
    "few commonly used clustering algorithms:\n",
    "\n",
    "1. **K-Means Clustering:** K-means is one of the most popular clustering algorithms. It partitions the data into K \n",
    "    clusters, where K is a user-defined parameter. The algorithm assigns each data point to the cluster whose mean\n",
    "    is closest to that point.\n",
    "\n",
    "2. **Hierarchical Clustering:** Hierarchical clustering builds a tree-like structure of clusters. It can be \n",
    "    agglomerative, where each data point starts in its own cluster, and pairs of clusters are merged as the \n",
    "    algorithm progresses, or divisive, where all data points start in one cluster and the algorithm recursively \n",
    "    splits them into smaller clusters.\n",
    "\n",
    "3. **DBSCAN (Density-Based Spatial Clustering of Applications with Noise):** DBSCAN groups together data points\n",
    "    that are closely packed together and marks data points as outliers if they lie alone in low-density regions. \n",
    "    It doesn't require specifying the number of clusters beforehand and can find arbitrarily shaped clusters.\n",
    "\n",
    "4. **Mean Shift Clustering:** Mean Shift is a non-parametric clustering algorithm that does not require prior\n",
    "    knowledge of the number of clusters. It works by iteratively shifting data points towards the mode (local maximum)\n",
    "    of the data distribution, ultimately converging to cluster centers.\n",
    "\n",
    "5. **Gaussian Mixture Model (GMM):** GMM is a probabilistic model that assumes that the data is generated from a\n",
    "    mixture of several Gaussian distributions with unknown parameters. It estimates these parameters to find the\n",
    "    underlying clusters in the data.\n",
    "\n",
    "6. **Agglomerative Clustering:** Agglomerative clustering starts with each data point as a separate cluster and\n",
    "    iteratively merges the closest pairs of clusters until only a single cluster remains.\n",
    "\n",
    "These algorithms have different strengths and weaknesses, making them suitable for various types of data and \n",
    "clustering tasks. The choice of a specific algorithm depends on the characteristics of the data and the goals of \n",
    "the analysis.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. What are some of the most popular clustering algorithm applications?\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "Clustering algorithms are widely used in various fields for different applications due to their ability to uncover\n",
    "hidden patterns and structures in data. Some of the most popular clustering algorithm applications include:\n",
    "\n",
    "1. **Image Segmentation:** Clustering algorithms can be used to segment images into distinct regions based on \n",
    "    similarities in pixel values. This is commonly used in medical image analysis, object recognition, and computer\n",
    "    vision tasks.\n",
    "\n",
    "2. **Customer Segmentation:** Businesses use clustering to group customers based on their purchasing behavior, \n",
    "    demographics, or preferences. This information is valuable for targeted marketing, personalized recommendations,\n",
    "    and customer relationship management.\n",
    "\n",
    "3. **Anomaly Detection:** Clustering can help identify anomalies or outliers in datasets. By clustering normal data\n",
    "    points together, any data points that do not belong to a cluster can be flagged as anomalies. This is used in \n",
    "    fraud detection, network security, and quality control.\n",
    "\n",
    "4. **Document Clustering:** Clustering algorithms can group similar documents together, enabling tasks such as topic\n",
    "    modeling, document organization, and information retrieval. It's particularly useful in text analysis and natural\n",
    "    language processing.\n",
    "\n",
    "5. **Recommendation Systems:** Clustering techniques are employed in collaborative filtering-based recommendation \n",
    "    systems. Users or items are clustered based on their behavior or features, and recommendations are made to users\n",
    "    based on the preferences of similar users or items within the same cluster.\n",
    "\n",
    "6. **Genomic Clustering:** In bioinformatics, clustering is used to analyze gene expression data, DNA sequences, \n",
    "    and protein interactions. It helps in identifying co-expressed genes, functional groups, and potential biomarkers\n",
    "    for diseases.\n",
    "\n",
    "7. **Spatial Data Analysis:** Clustering algorithms can be applied to spatial data, such as geographic coordinates, \n",
    "    to identify spatial patterns and group locations with similar characteristics. This is useful in urban planning,\n",
    "    environmental studies, and geospatial analysis.\n",
    "\n",
    "8. **Market Basket Analysis:** In retail, clustering is used to analyze customer purchase data to identify associations\n",
    "    between products that are frequently bought together. This information is valuable for optimizing store layouts, \n",
    "    product placements, and marketing strategies.\n",
    "\n",
    "9. **Speech and Speaker Recognition:** Clustering techniques are used in speech and speaker recognition systems to \n",
    "    group similar audio features and distinguish between different speakers or speech patterns.\n",
    "\n",
    "10. **Network Analysis:** Clustering algorithms can be applied to social networks, online communities, and communication\n",
    "    networks to identify communities, influential nodes, and patterns of interactions among entities.\n",
    "\n",
    "These applications demonstrate the versatility of clustering algorithms across diverse domains, making them essential \n",
    "tools in the field of data analysis and machine learning.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. When using K-Means, describe two strategies for selecting the appropriate number of clusters.\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "Selecting the appropriate number of clusters, often denoted as \\( k \\), is a crucial step when using the K-Means \n",
    "clustering algorithm. Choosing the right \\( k \\) value can significantly impact the quality of the clustering result.\n",
    "Here are two common strategies for selecting the appropriate number of clusters in K-Means:\n",
    "\n",
    "1. **Elbow Method:**\n",
    "\n",
    "   The elbow method is a heuristic used to determine the optimal number of clusters based on the within-cluster sum \n",
    "of squares (WCSS) values. WCSS represents the sum of squared distances between each data point in a cluster and the \n",
    "centroid of that cluster. Here's how the elbow method works:\n",
    "\n",
    "   - Compute K-Means clustering for a range of \\( k \\) values (for example, from 1 to \\( K_{\\text{max}} \\)).\n",
    "   - For each \\( k \\), calculate the WCSS.\n",
    "   - Plot the number of clusters (\\( k \\)) against the corresponding WCSS values.\n",
    "   - Look for the \"elbow point\" on the plot. The elbow point is the point where the rate of decrease in WCSS sharply\n",
    "changes. The idea is that adding more clusters beyond this point provides diminishing returns in terms of reducing WCSS.\n",
    "\n",
    "   Select the \\( k \\) value corresponding to the elbow point as the optimal number of clusters. However, it's important\n",
    "    to note that the elbow method is not always perfectly clear, especially if the plot doesn't have a distinct elbow.\n",
    "    In such cases, other methods might be considered.\n",
    "\n",
    "2. **Silhouette Score:**\n",
    "\n",
    "   The silhouette score measures how similar an object is to its own cluster compared to other clusters.\n",
    "A higher silhouette score indicates better-defined clusters. Here's how you can use the silhouette score to find \n",
    "the optimal number of clusters:\n",
    "\n",
    "   - Compute K-Means clustering for a range of \\( k \\) values.\n",
    "   - For each \\( k \\), calculate the average silhouette score across all data points. The silhouette score ranges\n",
    "    from -1 to 1. A high average silhouette score indicates that the object is well matched to its own cluster and \n",
    "    poorly matched to neighboring clusters.\n",
    "\n",
    "   Choose the \\( k \\) value that maximizes the average silhouette score as the optimal number of clusters.\n",
    "Unlike the elbow method, the silhouette score provides a more quantitative measure of the quality of clustering.\n",
    "\n",
    "Both methods provide insights into choosing an appropriate number of clusters, but it's advisable to use them together \n",
    "and consider the specific context of the problem to make an informed decision about the optimal \\( k \\) value for \n",
    "your K-Means clustering task.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4. What is mark propagation and how does it work? Why would you do it, and how would you do it?\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "Mark propagation, also known as label propagation or semi-supervised learning, is a machine learning technique used\n",
    "for both classification and clustering tasks. It's particularly useful when you have a small amount of labeled data \n",
    "and a larger amount of unlabeled data. Mark propagation leverages the information from the labeled data to make \n",
    "predictions or assign labels to the unlabeled data points.\n",
    "\n",
    "Here's how mark propagation works and why you might use it:\n",
    "\n",
    "### How Mark Propagation Works:\n",
    "\n",
    "1. **Initialization:** Start with a dataset where only a subset of the data points have labels (the labeled set),\n",
    "    and the rest are unlabeled. Each labeled data point is given a \"mark\" representing its label.\n",
    "\n",
    "2. **Propagation:** Propagate the marks from labeled data points to nearby unlabeled data points based on their\n",
    "    similarities. The intuition is that similar data points should have similar labels. Various similarity measures,\n",
    "    such as distance or feature similarity, can be used depending on the context of the problem.\n",
    "\n",
    "3. **Aggregation:** Aggregate the marks received from neighboring labeled data points to determine the label for\n",
    "    each unlabeled data point. This aggregation can involve averaging marks, considering the most common label \n",
    "    among neighbors, or using other aggregation strategies.\n",
    "\n",
    "4. **Iteration:** The process of propagation and aggregation can be repeated iteratively to refine the labels\n",
    "    assigned to the unlabeled data points. In each iteration, the marks are propagated and aggregated again.\n",
    "\n",
    "### Why Use Mark Propagation:\n",
    "\n",
    "1. **Limited Labeled Data:** Mark propagation is useful when you have a small amount of labeled data and a large\n",
    "    amount of unlabeled data. It allows you to leverage the limited labeled information to make predictions on a\n",
    "    much larger dataset.\n",
    "\n",
    "2. **Semi-Supervised Learning:** Mark propagation enables semi-supervised learning, where the model learns from \n",
    "    both labeled and unlabeled data. It can improve the performance of the model compared to using only the limited\n",
    "    labeled data.\n",
    "\n",
    "### How to Do Mark Propagation:\n",
    "\n",
    "1. **Similarity Metric:** Choose an appropriate similarity metric to measure the similarity between data points.\n",
    "    Common metrics include Euclidean distance, cosine similarity, or kernel-based measures, depending on the nature\n",
    "    of the data.\n",
    "\n",
    "2. **Propagation Rule:** Define rules for propagating marks from labeled to unlabeled data points. This can involve\n",
    "    weighted averaging, selecting the most similar neighbor, or other propagation strategies.\n",
    "\n",
    "3. **Iteration:** Decide on the number of iterations for mark propagation. It's often beneficial to perform several\n",
    "    iterations to refine the labels on unlabeled data points.\n",
    "\n",
    "4. **Evaluation:** Evaluate the performance of the model using metrics appropriate for the task, such as accuracy, \n",
    "    F1-score, or clustering evaluation metrics, depending on whether it's a classification or clustering problem.\n",
    "\n",
    "Mark propagation is a powerful technique, especially in scenarios where obtaining labeled data is expensive or\n",
    "time-consuming. However, it's important to carefully choose the similarity metrics, propagation rules, and the \n",
    "number of iterations to achieve optimal results.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "5. Provide two examples of clustering algorithms that can handle large datasets. And two that look\n",
    "for high-density areas?\n",
    "\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "Certainly! Here are two examples of clustering algorithms that are well-suited for handling large datasets, as well\n",
    "as two algorithms that are designed to identify high-density areas:\n",
    "\n",
    "### Clustering Algorithms for Large Datasets:\n",
    "\n",
    "1. **Mini-Batch K-Means:**\n",
    "   Mini-Batch K-Means is a variation of the traditional K-Means algorithm that is designed to handle large datasets\n",
    "efficiently. Instead of updating the centroids based on the entire dataset in each iteration, Mini-Batch K-Means \n",
    "randomly samples a subset (mini-batch) of the data and updates the centroids using this subset. This approach \n",
    "significantly reduces the computational cost, making it suitable for large datasets. While it may not converge\n",
    "to the optimal solution, it often provides a good approximation in a much shorter time.\n",
    "\n",
    "2. **DBSCAN (Density-Based Spatial Clustering of Applications with Noise):**\n",
    "   DBSCAN is a density-based clustering algorithm that can discover clusters of arbitrary shapes and handle noise \n",
    "in the data. It defines clusters as dense regions of data points separated by sparser regions. DBSCAN does not \n",
    "require specifying the number of clusters in advance and can handle large datasets effectively. It identifies \n",
    "clusters based on data density and is particularly useful for datasets where clusters have varying densities or \n",
    "when dealing with outliers.\n",
    "\n",
    "### Clustering Algorithms for High-Density Areas:\n",
    "\n",
    "1. **Mean Shift Clustering:**\n",
    "   Mean Shift is a non-parametric clustering algorithm that can identify clusters without assuming their shape or size.\n",
    "It works by iteratively shifting data points towards the mode (local maximum) of the data distribution. As a result,\n",
    "data points converge to high-density areas, forming clusters. Mean Shift can find arbitrarily shaped clusters and is\n",
    "effective in identifying high-density regions in the data.\n",
    "\n",
    "2. **OPTICS (Ordering Points To Identify the Clustering Structure):**\n",
    "   OPTICS is another density-based clustering algorithm that can find clusters of varying densities. It creates a\n",
    "reachability plot, which represents the density-based connectivity of the data points. By analyzing this plot, \n",
    "OPTICS can identify clusters and their hierarchical relationships. OPTICS is capable of discovering clusters in\n",
    "datasets with noise and varying densities, making it suitable for identifying high-density areas.\n",
    "\n",
    "Both Mean Shift and OPTICS are valuable for identifying high-density regions, and their ability to handle varying \n",
    "densities and noisy data makes them powerful tools in clustering applications where density variation is a key factor.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "6. Can you think of a scenario in which constructive learning will be advantageous? How can you go\n",
    "about putting it into action?\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "\n",
    "Constructive learning, also known as incremental learning or lifelong learning, refers to machine learning systems\n",
    "that have the ability to learn and adapt continuously over time as new data becomes available. This approach is\n",
    "advantageous in scenarios where:\n",
    "\n",
    "### Scenario: Adaptive Fraud Detection System\n",
    "\n",
    "Imagine a financial institution operating an online payment platform. The company faces an ever-evolving landscape\n",
    "of fraudulent activities, with new fraud patterns emerging regularly. A traditional, static fraud detection system\n",
    "might struggle to keep up with these dynamic patterns. Here's how constructive learning can be advantageous in this\n",
    "scenario:\n",
    "\n",
    "### Advantages of Constructive Learning:\n",
    "\n",
    "1. **Adaptability to New Patterns:** Constructive learning systems can continuously update their models as new data\n",
    "    arrives. In the context of fraud detection, this means the system can adapt to novel fraud patterns and tactics \n",
    "    employed by fraudsters. Traditional static models might miss these new patterns until the next scheduled update.\n",
    "\n",
    "2. **Real-time Response:** Constructive learning enables real-time learning and decision-making. As the system encounters\n",
    "    new, previously unseen fraudulent transactions, it can quickly incorporate this information into its model and improve \n",
    "    its accuracy in real time. This real-time response is crucial for preventing immediate financial losses.\n",
    "\n",
    "### Putting Constructive Learning into Action:\n",
    "\n",
    "1. **Data Collection:** Gather data on transactions, including both historical data and real-time transaction information.\n",
    "    This data should include features such as transaction amount, location, time, user behavior, and any other relevant\n",
    "    contextual information.\n",
    "\n",
    "2. **Incremental Model Training:** Implement an incremental machine learning algorithm that supports constructive learning.\n",
    "    Algorithms like online learning algorithms (e.g., Online Gradient Descent) are designed to learn from new data \n",
    "    points without retraining on the entire dataset. These algorithms update the model iteratively as new data arrives.\n",
    "\n",
    "3. **Feature Engineering:** Continuously evaluate and update the features used in the model. Introduce new features \n",
    "    that might capture emerging fraud patterns. Feature engineering is essential in adapting the model to new types \n",
    "    of fraud.\n",
    "\n",
    "4. **Monitoring and Feedback Loop:** Implement a monitoring system that tracks the system's performance in real time.\n",
    "    When the system encounters a potentially new fraud pattern, it should be flagged for human review. Feedback from\n",
    "    human experts can be used to validate the pattern and, if confirmed as fraud, incorporate it into the model for \n",
    "    future predictions.\n",
    "\n",
    "5. **Regular Evaluation and Retraining:** Periodically evaluate the model's performance on a validation dataset. \n",
    "    If there is a drop in accuracy or an increase in false negatives (missed fraud cases), retrain the model using\n",
    "    the new data and the updated features. This continuous feedback loop ensures that the model remains effective over time.\n",
    "    \n",
    "\n",
    "By employing constructive learning in this scenario, the financial institution can build a fraud detection system that \n",
    "not only adapts to new fraud patterns but also provides a more robust and efficient defense against evolving threats in\n",
    "real time.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "7. How do you tell the difference between anomaly and novelty detection?\n",
    "\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "\n",
    "Anomaly detection and novelty detection are both techniques used in machine learning to identify unusual patterns\n",
    "or observations in data. While they are related concepts, they have subtle differences in their objectives and applications:\n",
    "\n",
    "### Anomaly Detection:\n",
    "\n",
    "**Objective:** Anomaly detection, also known as outlier detection, focuses on identifying data points that deviate \n",
    "    significantly from the majority of the data. These are rare instances that do not conform to the normal behavior\n",
    "    of the dataset.\n",
    "\n",
    "**Use Case:** Anomaly detection is used when the goal is to find abnormal or suspicious patterns that are different\n",
    "    from the majority of the data. Common applications include fraud detection, network security, quality control, \n",
    "    and identifying defective products in manufacturing.\n",
    "\n",
    "**Training Data:** Anomaly detection algorithms typically require a dataset that contains both normal (inlier) and \n",
    "    anomalous (outlier) examples during the training phase. The algorithm learns the patterns of normal behavior and\n",
    "    flags instances that deviate significantly from this learned pattern as anomalies.\n",
    "\n",
    "### Novelty Detection:\n",
    "\n",
    "**Objective:** Novelty detection, on the other hand, is focused on identifying new, unseen patterns or outliers that\n",
    "    differ from the training dataset. It aims to detect novel patterns that were not present in the training data.\n",
    "\n",
    "**Use Case:** Novelty detection is used in scenarios where the goal is to recognize new, previously unseen instances\n",
    "    or patterns that do not match the learned patterns from the training data. Applications include intrusion \n",
    "    detection in computer networks, identifying new types of fraud, and detecting emerging trends in data.\n",
    "\n",
    "**Training Data:** Novelty detection algorithms are trained only on normal (inlier) data. They learn the normal \n",
    "    patterns and aim to identify instances that significantly differ from what they have learned. Unlike anomaly \n",
    "    detection, novelty detection algorithms do not have access to labeled anomalous data during training.\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "1. **Training Data:** Anomaly detection algorithms require both normal and anomalous data for training, whereas\n",
    "    novelty detection algorithms are trained only on normal data.\n",
    "\n",
    "2. **Objective:** Anomaly detection aims to identify both known and unknown anomalies within the dataset. Novelty\n",
    "    detection specifically focuses on detecting new, previously unseen patterns that were not part of the training data.\n",
    "\n",
    "3. **Use Case:** Anomaly detection is suitable for scenarios where you want to identify any abnormal patterns,\n",
    "    whether known or unknown. Novelty detection is specifically designed for scenarios where the goal is to identify \n",
    "    novel, previously unseen instances or patterns.\n",
    "\n",
    "In summary, while both anomaly detection and novelty detection deal with identifying unusual patterns, their training \n",
    "data and objectives differ, making them suitable for different applications and scenarios.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "8. What is a Gaussian mixture, and how does it work? What are some of the things you can do about\n",
    "it?\n",
    "\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "Using **two different dimensionality reduction algorithms in a chain** can be a valid approach, but it depends on the \n",
    "context and goals of your analysis. Let's explore some considerations:\n",
    "\n",
    "1. **Complementary Strengths**: If the two algorithms have complementary strengths, combining them might lead to better\n",
    "    results. For example, one algorithm might be better at capturing linear relationships, while the other excels at\n",
    "    capturing non-linear patterns. By chaining them, you can benefit from both aspects.\n",
    "\n",
    "2. **Sequential Approach**: You can think of it as a **sequential approach**. The first algorithm reduces the \n",
    "    dimensionality to an intermediate space, and the second algorithm further reduces it. This can be useful when\n",
    "    the original data is high-dimensional and complex.\n",
    "\n",
    "3. **Trade-offs and Complexity**: However, there are trade-offs. Chaining algorithms can increase computational \n",
    "    complexity and introduce more hyperparameters to tune. It's essential to consider the computational cost and \n",
    "    whether the additional complexity is justified.\n",
    "\n",
    "4. **Risk of Overfitting**: Be cautious about **overfitting**. If the first algorithm overfits the data, \n",
    "    the subsequent algorithm might amplify this effect. Regularization techniques or cross-validation can help\n",
    "    mitigate this risk.\n",
    "\n",
    "5. **Interpretability**: Consider the interpretability of the final representation. If interpretability is crucial,\n",
    "    ensure that the combined approach doesn't obscure meaningful patterns.\n",
    "\n",
    "6. **Empirical Evaluation**: Experiment with different combinations and evaluate their performance on your specific task. \n",
    "    Use metrics like reconstruction error, clustering quality, or downstream performance to assess the effectiveness\n",
    "    of the chain.\n",
    "\n",
    "In summary, while using two different dimensionality reduction algorithms in a chain is logical in some scenarios,\n",
    "it's essential to weigh the benefits against the added complexity and potential risks. Always validate your approach\n",
    "empirically to ensure it aligns with your goals. \n",
    "\n",
    "\n",
    "\n",
    "9. When using a Gaussian mixture model, can you name two techniques for determining the correct\n",
    "number of clusters?\n",
    "\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Certainly! When working with a **Gaussian mixture model (GMM)**, determining the optimal number of clusters is crucial.\n",
    "Here are two techniques to help you decide:\n",
    "\n",
    "1. **Bayesian Information Criterion (BIC)**:\n",
    "   - BIC is a statistical criterion that balances model fit and complexity. It penalizes models with more parameters.\n",
    "     For GMMs, BIC can be used to assess the number of components (clusters).\n",
    "   - The idea is to find the number of clusters that minimizes the BIC value. Lower BIC indicates a better trade-off\n",
    "     between goodness of fit and model complexity.\n",
    "   - In practice, you fit GMMs with different numbers of components (e.g., from 1 to K), compute their BIC values, \n",
    "     and choose the one with the lowest BIC.\n",
    "\n",
    "2. **Cross-Validation**:\n",
    "   - Cross-validation helps estimate the performance of a model on unseen data. For GMMs, you can use techniques like\n",
    "     **K-fold cross-validation**.\n",
    "   - Split your data into K folds, train GMMs with different numbers of components on K-1 folds, and evaluate their performance\n",
    "     on the remaining fold.\n",
    "   - Repeat this process for different numbers of components and choose the one that performs well across all folds.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
