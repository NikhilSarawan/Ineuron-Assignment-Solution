{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7739adc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What are Vanilla autoencoders\n",
    "\n",
    "Ans-\n",
    "\n",
    "**Vanilla Autoencoders:**\n",
    "   \n",
    "Vanilla autoencoders are a type of neural network architecture used for unsupervised learning and dimensionality reduction. \n",
    "The basic idea behind an autoencoder is to learn a compressed, efficient representation (encoding) of input data. \n",
    "It consists of two main parts:\n",
    "\n",
    "1. **Encoder:** This part of the network compresses the input data into a lower-dimensional representation. It takes \n",
    "    the input and transforms it into a code or latent space representation.\n",
    "\n",
    "2. **Decoder:** This part of the network reconstructs the input data from the encoded representation. It takes the encoded\n",
    "    data and tries to generate an output that is as close as possible to the original input.\n",
    "\n",
    "The training objective of a vanilla autoencoder is to minimize the reconstruction error, which is the difference between \n",
    "the input data and the reconstructed output. Autoencoders can be used for various tasks, such as data compression, denoising,\n",
    "and feature learning.\n",
    "\n",
    "The term \"vanilla\" is used to distinguish this basic form of autoencoder from variations that incorporate additional \n",
    "constraints or modifications for specific purposes, such as sparse autoencoders, denoising autoencoders, or convolutional\n",
    "autoencoders.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. What are Sparse autoencoders\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "\n",
    "**Sparse Autoencoders:**\n",
    "\n",
    "Sparse autoencoders are a variation of autoencoder neural networks that introduce sparsity constraints in the hidden\n",
    "layer's representation. The purpose of imposing sparsity is to encourage the model to learn a more compact and meaningful \n",
    "representation of the input data. In a standard autoencoder, the hidden layer might learn redundant or unnecessary features,\n",
    "but by incorporating sparsity, the model is encouraged to activate only a subset of neurons for a given input.\n",
    "\n",
    "Key characteristics of sparse autoencoders:\n",
    "\n",
    "1. **Sparsity Constraint:** A penalty term is added to the loss function during training to encourage a sparse activation \n",
    "    of neurons in the hidden layer. This constraint can be implemented using techniques like L1 regularization, which adds\n",
    "    a penalty proportional to the absolute values of the weights.\n",
    "\n",
    "2. **Kullback-Leibler (KL) Divergence:** In the context of sparse autoencoders, the KL divergence is often used to measure\n",
    "    the sparsity of the activation. The network aims to minimize the KL divergence between the desired sparsity and the\n",
    "    actual sparsity of the hidden layer.\n",
    "\n",
    "Sparse autoencoders are useful for feature learning and extraction, where the goal is to discover and capture the most \n",
    "relevant and discriminative features in the input data. They have applications in various domains, including image \n",
    "recognition, natural language processing, and signal processing.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. What are Denoising autoencoders\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "\n",
    "**Denoising Autoencoders:**\n",
    "\n",
    "Denoising autoencoders are a type of autoencoder designed to learn robust representations of data by training on corrupted\n",
    "versions of the input. The primary objective is to force the model to capture the essential features of the data, even in \n",
    "the presence of noise or corruption. Denoising autoencoders are particularly useful for tasks where the input data is noisy,\n",
    "incomplete, or subject to distortions.\n",
    "\n",
    "Key features of denoising autoencoders:\n",
    "\n",
    "1. **Corrupted Input:** During training, the input data is intentionally corrupted by adding noise or introducing distortions.\n",
    "    This could involve randomly zeroing out some features, adding Gaussian noise, or applying other types of perturbations.\n",
    "\n",
    "2. **Reconstruction Objective:** The network is trained to reconstruct the original, uncorrupted input from the noisy or \n",
    "    corrupted version. The objective is to minimize the difference between the reconstructed data and the clean input.\n",
    "\n",
    "3. **Robust Feature Learning:** Denoising autoencoders aim to learn features that are robust to variations and noise in\n",
    "    the input. By exposing the model to various corrupted samples, it learns to capture the underlying structure of the data.\n",
    "\n",
    "Denoising autoencoders find applications in image denoising, signal processing, and any domain where the input data is\n",
    "prone to corruption or noise. They can also be used for feature learning in scenarios where the data is inherently noisy,\n",
    "helping the model generalize better to unseen, noisy examples.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4. What are Convolutional autoencoders\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "**Convolutional Autoencoders:**\n",
    "\n",
    "Convolutional autoencoders are a type of autoencoder architecture that leverages convolutional layers for both the encoder\n",
    "and decoder components. These autoencoders are particularly well-suited for handling grid-structured data, such as images,\n",
    "where local patterns and spatial relationships are crucial.\n",
    "\n",
    "Key features of convolutional autoencoders:\n",
    "\n",
    "1. **Convolutional Layers:** Instead of using fully connected layers, convolutional autoencoders employ convolutional\n",
    "    layers in the encoder and decoder. Convolutional layers are effective for capturing local patterns and hierarchical\n",
    "    features in grid-like data.\n",
    "\n",
    "2. **Pooling Layers:** Pooling layers, such as max pooling or average pooling, are often used to downsample the spatial\n",
    "    dimensions of the input in the encoder, reducing the computational load and focusing on the most relevant information.\n",
    "\n",
    "3. **Transposed Convolution (Deconvolution):** In the decoder, transposed convolutional layers\n",
    "    (also known as deconvolutional layers) are used to upsample the spatial dimensions.\n",
    "    These layers help reconstruct the original spatial structure of the input.\n",
    "\n",
    "4. **Reconstruction Objective:** Like other autoencoders, the goal is to minimize the reconstruction error between \n",
    "    the input and the reconstructed output. Convolutional autoencoders excel in capturing spatial dependencies and \n",
    "    are well-suited for tasks such as image denoising, compression, and feature learning.\n",
    "\n",
    "Convolutional autoencoders are widely used in computer vision applications, including image generation, image-to-image \n",
    "translation, and feature extraction from images. They are effective in learning hierarchical representations of visual \n",
    "data by exploiting the locality and translational invariance properties of convolutional operations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "5. What are Stacked autoencoders\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "\n",
    "**Stacked Autoencoders:**\n",
    "\n",
    "Stacked autoencoders, also known as deep autoencoders or multilayer autoencoders, are a type of autoencoder architecture \n",
    "that consists of multiple layers of encoder and decoder units stacked on top of each other. Each layer in the stack serves\n",
    "as the encoder for the layer above it and the decoder for the layer below it. The result is a deep neural network that can\n",
    "learn hierarchical representations of the input data.\n",
    "\n",
    "Key features of stacked autoencoders:\n",
    "\n",
    "1. **Depth:** Stacked autoencoders have more than one hidden layer, allowing them to capture hierarchical features and \n",
    "    complex patterns in the input data. The hidden layers progressively learn more abstract and higher-level representations.\n",
    "\n",
    "2. **Layer-wise Pretraining:** Training a deep autoencoder from scratch can be challenging due to issues like vanishing\n",
    "    gradients. Stacked autoencoders are often trained in a layer-wise fashion. Each layer is pretrained as an individual \n",
    "    autoencoder, and then the entire stack is fine-tuned jointly.\n",
    "\n",
    "3. **Reconstruction Objective:** Like other autoencoders, the training objective is to minimize the reconstruction error\n",
    "    between the input and the reconstructed output. The model learns to encode and decode the data through the stacked layers, \n",
    "    capturing increasingly abstract features.\n",
    "\n",
    "4. **Nonlinear Activation Functions:** Stacked autoencoders typically use nonlinear activation functions such as ReLU \n",
    "    (Rectified Linear Unit) or sigmoid to introduce nonlinearity into the network, enabling it to learn complex mappings.\n",
    "\n",
    "Stacked autoencoders are powerful for learning hierarchical representations and have been successfully applied to various \n",
    "tasks, including image recognition, natural language processing, and feature learning. They are a key component in the \n",
    "success of deep learning for a wide range of applications.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "6. Explain how to generate sentences using LSTM autoencoders\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "\n",
    "Generating sentences using LSTM (Long Short-Term Memory) autoencoders involves training a neural network to encode and \n",
    "decode sequences of words. Autoencoders are designed to learn a compact representation of the input data, and in the case\n",
    "of LSTM autoencoders, they are well-suited for sequence data like sentences.\n",
    "\n",
    "Here's a general overview of the process:\n",
    "\n",
    "1. **Data Preparation:**\n",
    "   - Prepare a dataset of sentences that you want the LSTM autoencoder to learn from. Tokenize the sentences into words \n",
    "      or subword units.\n",
    "   - Convert the words or subword units into numerical representations, such as word embeddings, so that they can be fed \n",
    "     into the neural network.\n",
    "\n",
    "2. **Model Architecture:**\n",
    "   - Design an LSTM autoencoder architecture. The encoder part of the network processes the input sequence and produces a \n",
    "compressed representation, while the decoder part reconstructs the original sequence from this representation.\n",
    "   - Both the encoder and decoder should use LSTM layers to capture sequential dependencies.\n",
    "\n",
    "3. **Training:**\n",
    "   - Train the LSTM autoencoder on your dataset. The training objective is to minimize the difference between the input\n",
    "sequence and the reconstructed sequence.\n",
    "   - Use a suitable loss function, such as mean squared error or categorical cross-entropy, depending on the nature of\n",
    "    your data and the task.\n",
    "\n",
    "4. **Sequence Generation:**\n",
    "   - Once the LSTM autoencoder is trained, you can use the decoder part to generate new sentences.\n",
    "   - Provide an initial input sequence (it could be a partial or complete sentence) to the decoder and let the LSTM \n",
    "    generate the next word in the sequence.\n",
    "   - Repeat the process, using the generated words as input for subsequent steps, until you reach a desired length or \n",
    "generate an end-of-sequence token.\n",
    "\n",
    "5. **Temperature Sampling (Optional):**\n",
    "   - Optionally, you can introduce temperature sampling during sequence generation. Temperature controls the randomness \n",
    "of the generated sequences. Higher temperatures result in more diverse but potentially less coherent output, while lower \n",
    "temperatures produce more focused but less diverse output.\n",
    "\n",
    "6. **Evaluation and Fine-Tuning:**\n",
    "   - Evaluate the generated sequences based on your specific criteria, such as coherence, relevance, or grammaticality.\n",
    "   - Fine-tune the LSTM autoencoder or adjust the training parameters based on the quality of the generated sequences.\n",
    "\n",
    "This process is a simplified overview, and the details may vary based on the specific architecture, dataset, and objectives\n",
    "of your LSTM autoencoder for sentence generation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "7. Explain Extractive summarization\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "\n",
    "**Extractive Summarization:**\n",
    "\n",
    "Extractive summarization is a text summarization technique where the goal is to generate a concise summary by selecting \n",
    "and extracting the most important sentences or phrases from the original text. Instead of generating new sentences,\n",
    "extractive summarization identifies and pulls out existing content deemed essential to convey the main ideas of the text.\n",
    "\n",
    "Here are the key steps and characteristics of extractive summarization:\n",
    "\n",
    "1. **Sentence Ranking:**\n",
    "   - The sentences in the original text are ranked based on their importance or relevance to the overall content.\n",
    "Various methods can be employed for sentence ranking, such as using machine learning algorithms, graph-based algorithms,\n",
    "or statistical approaches.\n",
    "\n",
    "2. **Feature Extraction:**\n",
    "   - Features for ranking sentences can include word frequency, term importance, sentence length, and other linguistic\n",
    "features. The goal is to identify the sentences that contain the most critical information.\n",
    "\n",
    "3. **Scoring and Selection:**\n",
    "   - Each sentence is assigned a score based on the extracted features, and the sentences with the highest scores are\n",
    "selected for inclusion in the summary. The selection process may involve setting a threshold or selecting a fixed number\n",
    "of top-ranked sentences.\n",
    "\n",
    "4. **Creation of Summary:**\n",
    "   - The selected sentences are combined to form the extractive summary. This summary is a subset of the original sentences\n",
    "and is intended to capture the main points of the text.\n",
    "\n",
    "Advantages of Extractive Summarization:\n",
    "\n",
    "- **Preservation of Source Language:**\n",
    "  Extractive summarization directly uses sentences from the source text, preserving the language style and expressions \n",
    "present in the original content.\n",
    "\n",
    "- **Reduced Ambiguity:**\n",
    "  Since extractive summarization pulls sentences directly from the source, there is less chance of introducing errors \n",
    "or generating ambiguous language.\n",
    "\n",
    "- **Suitability for Specific Domains:**\n",
    "  Extractive summarization can be effective in specific domains where the content is technical or domain-specific, \n",
    "as it relies on selecting sentences that convey specialized information.\n",
    "\n",
    "However, extractive summarization has limitations, such as potential redundancy in the summary and the challenge of \n",
    "dealing with diverse writing styles. Despite these limitations, it remains a widely used approach for summarizing \n",
    "long documents or articles.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "8. Explain Abstractive summarization\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "\n",
    "**Abstractive Summarization:**\n",
    "\n",
    "Abstractive summarization is a text summarization technique where the goal is to generate a concise summary that captures \n",
    "the main ideas of the original text in a new and potentially more concise way. Unlike extractive summarization,\n",
    "abstractive summarization involves creating novel sentences that may not exist verbatim in the source document. \n",
    "This process requires a deeper understanding of the content and the ability to generate language creatively.\n",
    "\n",
    "Key characteristics and steps involved in abstractive summarization:\n",
    "\n",
    "1. **Understanding and Representation:**\n",
    "   - The system needs to understand the meaning and context of the input text. This often involves using natural \n",
    "language processing (NLP) techniques, such as parsing, semantic analysis, and entity recognition, to create a \n",
    "representation of the text.\n",
    "\n",
    "2. **Content Transformation:**\n",
    "   - The system generates a condensed version of the input text by transforming and rephrasing the content.\n",
    "This involves synthesizing new sentences that convey the essential information while potentially using different\n",
    "wording and structures.\n",
    "\n",
    "3. **Language Generation:**\n",
    "   - Abstractive summarization systems employ natural language generation techniques, such as neural language models\n",
    "or rule-based methods, to create coherent and contextually appropriate sentences for the summary.\n",
    "\n",
    "4. **Optimization and Evaluation:**\n",
    "   - The generated summary is often refined through optimization processes to improve its coherence, fluency, and \n",
    "informativeness. Evaluation metrics, such as ROUGE (Recall-Oriented Understudy for Gisting Evaluation), are commonly\n",
    "used to assess the quality of abstractive summaries.\n",
    "\n",
    "Advantages of Abstractive Summarization:\n",
    "\n",
    "- **Creativity and Novelty:**\n",
    "  Abstractive summarization has the ability to generate novel sentences that may not exist in the source document,\n",
    "allowing for more creative and concise summaries.\n",
    "\n",
    "- **Condensation of Information:**\n",
    "  By rewriting and rephrasing content, abstractive summarization can produce more concise summaries that capture the\n",
    "key information without unnecessary details.\n",
    "\n",
    "- **Handling of Redundancy:**\n",
    "  Abstractive summarization can potentially avoid redundancy present in the source text by generating more concise\n",
    "and focused sentences.\n",
    "\n",
    "However, abstractive summarization is a challenging task due to the need for advanced natural language understanding\n",
    "and generation capabilities. It often requires sophisticated neural network architectures, such as sequence-to-sequence\n",
    "models with attention mechanisms, to achieve high-quality abstractive summaries.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "9. Explain Beam search\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "\n",
    "\n",
    "**Beam Search:**\n",
    "\n",
    "Beam search is a search algorithm used in natural language processing and machine translation for generating sequences of words, such as sentences or captions. It is commonly employed in tasks like language modeling, sequence-to-sequence learning, and text generation. Beam search is an optimization technique that improves the quality of generated sequences by considering multiple candidate sequences simultaneously.\n",
    "\n",
    "Here's how beam search typically works:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Begin with an initial seed sequence (e.g., the start token) and generate a set of candidate sequences based on the\n",
    "probabilities of the next words.\n",
    "\n",
    "2. **Expanding Candidates:**\n",
    "   - For each candidate sequence, generate a set of possible next words and append them to the respective candidate\n",
    "sequences. Assign probabilities to each extended sequence based on the likelihood of the chosen next words.\n",
    "\n",
    "3. **Selection:**\n",
    "   - Select the top-k sequences with the highest probabilities, where \"k\" is a user-defined parameter known as the\n",
    "\"beam width\" or \"beam size.\" These top-k sequences become the candidates for the next iteration.\n",
    "\n",
    "4. **Repeat:**\n",
    "   - Repeat the process for a fixed number of iterations or until a stopping criterion is met. At each step, the \n",
    "number of candidate sequences remains constant at k.\n",
    "\n",
    "5. **Termination:**\n",
    "   - End the search when a termination condition is met, such as generating an end-of-sequence token or reaching a\n",
    "maximum sequence length.\n",
    "\n",
    "The primary benefit of beam search is that it explores multiple possible continuations of a sequence, allowing the\n",
    "model to consider alternative options beyond the most probable one at each step. This helps mitigate issues like \n",
    "getting stuck in a locally optimal choice and promotes more diverse and contextually coherent output.\n",
    "\n",
    "However, beam search has some limitations:\n",
    "\n",
    "- **Suboptimal Solutions:** While beam search improves the quality of generated sequences, it does not guarantee \n",
    "    finding the globally optimal solution. The top-k candidates at each step may collectively miss the best overall \n",
    "    sequence.\n",
    "\n",
    "- **Redundancy:** Beam search may lead to redundant or similar sequences in the final output, as the model tends to\n",
    "    explore similar paths.\n",
    "\n",
    "Researchers often experiment with variations of beam search and other decoding strategies to balance the trade-offs \n",
    "and address its limitations, such as nucleus sampling or diverse beam search.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "10. Explain Length normalization\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "**Length Normalization:**\n",
    "\n",
    "Length normalization is a technique used in natural language processing, particularly in the context of sequence\n",
    "generation tasks, to mitigate biases introduced by the length of generated sequences. When using machine learning models, \n",
    "such as neural networks, for tasks like text generation, the length of the generated output can influence the training \n",
    "and decoding processes. Length normalization is applied to ensure fairness and prevent length-related biases.\n",
    "\n",
    "In the context of sequence generation, including machine translation and text summarization, length normalization is\n",
    "often used during the decoding or inference phase. The goal is to produce sequences that are not favored or penalized\n",
    "based solely on their length.\n",
    "\n",
    "Here's a simplified explanation of how length normalization is applied:\n",
    "\n",
    "1. **Objective Function Adjustment:**\n",
    "   - In many sequence generation tasks, the model is trained to optimize an objective function, such as cross-entropy \n",
    "loss. During training, the model learns to generate sequences that maximize the likelihood of the target sequences.\n",
    "\n",
    "2. **Length Penalty Term:**\n",
    "   - Length normalization introduces a penalty term based on the length of the generated sequence. This penalty is\n",
    "applied during decoding to adjust the scores or probabilities assigned to candidate sequences.\n",
    "\n",
    "3. **Normalization Factor:**\n",
    "   - The penalty term is often formulated as a function of the length of the sequence, typically divided by a\n",
    "normalization factor. The normalization factor ensures that the penalty is not too strong or too weak, and it is\n",
    "chosen based on the characteristics of the task and the desired behavior of the model.\n",
    "\n",
    "4. **Final Score Adjustment:**\n",
    "   - The adjusted scores, incorporating the length penalty, are used for ranking and selecting the final output \n",
    "sequence during decoding. This helps in avoiding a bias toward shorter or longer sequences.\n",
    "\n",
    "The specific form of the length penalty term can vary. A commonly used formulation is to add a factor proportional \n",
    "to the length of the generated sequence to the log-likelihood or other scoring function. The length penalty encourages \n",
    "the model to generate sequences that are not excessively short or long, promoting a balance between informativeness \n",
    "and conciseness.\n",
    "\n",
    "Length normalization is crucial in tasks where the length of the generated output can significantly impact the\n",
    "evaluation metrics. It is often used in combination with other decoding strategies, such as beam search, to enhance\n",
    "the quality and diversity of generated sequences.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "11. Explain Coverage normalization\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "\n",
    "**Coverage Normalization:**\n",
    "\n",
    "Coverage normalization is a technique used in sequence-to-sequence models, especially in the context of abstractive \n",
    "summarization and machine translation. The goal of coverage normalization is to address the issue of repetitiveness \n",
    "in generated sequences by encouraging the model to attend to different parts of the input sequence during the generation\n",
    "of each token. It helps in mitigating the problem where the model focuses too much on certain words or phrases and \n",
    "neglects other important content.\n",
    "\n",
    "Here's an overview of how coverage normalization works:\n",
    "\n",
    "1. **Attention Mechanism:**\n",
    "   - Sequence-to-sequence models, particularly those using attention mechanisms, generate output tokens by attending \n",
    "     to different parts of the input sequence. Attention scores indicate the relevance of each input token to the generation\n",
    "     of the current output token.\n",
    "\n",
    "2. **Coverage Vector:**\n",
    "   - A coverage vector is maintained during the decoding process. This vector keeps track of the attention history by\n",
    "     accumulating attention scores over time. Each element in the coverage vector corresponds to an input token, and it is \n",
    "     updated at each decoding step.\n",
    "\n",
    "3. **Attention Calculation:**\n",
    "   - When calculating attention scores for the next token, the coverage vector is considered along with the standard\n",
    "     attention mechanism. The coverage vector is used to penalize tokens that have been attended to in previous decoding\n",
    "     steps.\n",
    "\n",
    "4. **Coverage Penalty:**\n",
    "   - A coverage penalty term is introduced in the model's objective function. This penalty encourages the model to \n",
    "     attend to tokens that have not been covered sufficiently, promoting diversity in the attention distribution.\n",
    "\n",
    "5. **Normalization Factor:**\n",
    "   - The coverage penalty is scaled by a normalization factor to ensure that its impact is balanced with other components \n",
    "     of the objective function. The normalization factor is typically chosen based on the characteristics of the task.\n",
    "\n",
    "By incorporating coverage normalization, the model is guided to distribute attention more evenly across the input sequence,\n",
    "reducing the likelihood of repetitive or redundant output. This is particularly important in tasks like abstractive \n",
    "summarization, where generating concise and diverse summaries is crucial.\n",
    "\n",
    "Coverage normalization is an extension of the attention mechanism and is often used in combination with other techniques,\n",
    "such as length normalization and diverse decoding strategies, to improve the overall quality and diversity of generated \n",
    "sequences.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "12. Explain ROUGE metric evaluation\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "**ROUGE Metric Evaluation:**\n",
    "\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used for automatic evaluation of \n",
    "machine-generated text, especially in the context of text summarization and machine translation. The ROUGE metrics \n",
    "assess the quality of generated summaries or translations by comparing them to reference (human-generated) summaries.\n",
    "\n",
    "There are several variants of ROUGE metrics, but the most commonly used ones include ROUGE-N, ROUGE-L, and ROUGE-W:\n",
    "\n",
    "1. **ROUGE-N (N-gram Overlap):**\n",
    "   - ROUGE-N measures the overlap of n-grams (contiguous sequences of n words) between the generated summary and the\n",
    "reference summary. It includes unigrams (ROUGE-1), bigrams (ROUGE-2), trigrams (ROUGE-3), etc.\n",
    "   - Example: ROUGE-2 measures the overlap of bigrams between the generated and reference summaries.\n",
    "\n",
    "2. **ROUGE-L (Longest Common Subsequence):**\n",
    "   - ROUGE-L measures the longest common subsequence (LCS) between the generated summary and the reference summary. \n",
    "It evaluates the length of the shared sequence of words.\n",
    "   - Example: If the LCS is long, it suggests that the generated summary captures the main content of the reference \n",
    "        summary.\n",
    "\n",
    "3. **ROUGE-W (Weighted LCS):**\n",
    "   - ROUGE-W is an extension of ROUGE-L that incorporates word-level weighting. It assigns different weights to words \n",
    "based on their importance in the sentences.\n",
    "   - Example: Words in the LCS that are deemed more important contribute more to the final ROUGE-W score.\n",
    "\n",
    "**How ROUGE is Calculated:**\n",
    "- For ROUGE-N, it calculates precision, recall, and F1-score based on n-gram overlap.\n",
    "- For ROUGE-L, it computes precision, recall, and F1-score based on the length of the LCS.\n",
    "- For ROUGE-W, it considers word-level weights in addition to the LCS.\n",
    "\n",
    "**Interpretation:**\n",
    "- Precision measures how much of the generated content is also in the reference.\n",
    "- Recall measures how much of the reference content is covered by the generated summary.\n",
    "- F1-score is the harmonic mean of precision and recall.\n",
    "\n",
    "**Use Cases:**\n",
    "- ROUGE metrics are commonly used in research to compare the quality of different summarization or translation systems.\n",
    "- They are used in competitions and evaluations, such as the Document Understanding Conference (DUC) and the Text Analysis \n",
    "Conference (TAC).\n",
    "\n",
    "It's important to note that while ROUGE metrics provide automated evaluation, they have limitations. They focus on \n",
    "surface-level matching and may not capture the semantic quality or fluency of generated text. As with any evaluation \n",
    "metric, a comprehensive assessment of model performance should include multiple metrics and human evaluation.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
