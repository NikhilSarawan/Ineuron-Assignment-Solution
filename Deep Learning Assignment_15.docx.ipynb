{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a86c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Deep Learning.\n",
    "a. Build a DNN with five hidden layers of 100 neurons each, He initialization, and the\n",
    "ELU activation function.\n",
    "b. Using Adam optimization and early stopping, try training it on MNIST but only on\n",
    "digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You\n",
    "will need a softmax output layer with five neurons, and as always make sure to save\n",
    "checkpoints at regular intervals and save the final model so you can reuse it later.\n",
    "c. Tune the hyperparameters using cross-validation and see what precision you can\n",
    "achieve.\n",
    "d. Now try adding Batch Normalization and compare the learning curves: is it\n",
    "converging faster than before? Does it produce a better model?\n",
    "e. Is the model overfitting the training set? Try adding dropout to every layer and try\n",
    "again. Does it help?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "\n",
    "Certainly! Here's a step-by-step guide to implement the tasks you mentioned:\n",
    "\n",
    "### a. Build a DNN with specified architecture and initialization:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, ELU\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load MNIST data for digits 0 to 4\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_mask = (train_labels < 5)\n",
    "test_mask = (test_labels < 5)\n",
    "train_images, train_labels = train_images[train_mask], train_labels[train_mask]\n",
    "test_images, test_labels = test_images[test_mask], test_labels[test_mask]\n",
    "\n",
    "# Preprocess the data\n",
    "train_images = train_images.reshape((train_images.shape[0], -1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((test_images.shape[0], -1)).astype('float32') / 255\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='elu', kernel_initializer=HeNormal(), input_shape=(784,)))\n",
    "model.add(Dense(100, activation='elu', kernel_initializer=HeNormal()))\n",
    "model.add(Dense(100, activation='elu', kernel_initializer=HeNormal()))\n",
    "model.add(Dense(100, activation='elu', kernel_initializer=HeNormal()))\n",
    "model.add(Dense(100, activation='elu', kernel_initializer=HeNormal()))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "```\n",
    "\n",
    "### b. Compile the model and set up optimization and early stopping:\n",
    "\n",
    "```python\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Set up callbacks for early stopping and model checkpoints\n",
    "early_stopping_cb = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "checkpoint_cb = ModelCheckpoint(\"model.h5\", save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, epochs=100, validation_split=0.2, \n",
    "callbacks=[early_stopping_cb, checkpoint_cb])\n",
    "```\n",
    "\n",
    "### c. Tune hyperparameters using cross-validation:\n",
    "\n",
    "For hyperparameter tuning, you can use techniques like Grid Search or Random Search along with\n",
    "cross-validation to find the optimal set of hyperparameters for your model.\n",
    "\n",
    "### d. Add Batch Normalization and compare learning curves:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Build the model with Batch Normalization\n",
    "model_bn = Sequential()\n",
    "model_bn.add(Dense(100, input_shape=(784,)))\n",
    "model_bn.add(BatchNormalization())\n",
    "model_bn.add(ELU())\n",
    "model_bn.add(Dense(100))\n",
    "model_bn.add(BatchNormalization())\n",
    "model_bn.add(ELU())\n",
    "model_bn.add(Dense(100))\n",
    "model_bn.add(BatchNormalization())\n",
    "model_bn.add(ELU())\n",
    "model_bn.add(Dense(100))\n",
    "model_bn.add(BatchNormalization())\n",
    "model_bn.add(ELU())\n",
    "model_bn.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile and train the model with Batch Normalization\n",
    "model_bn.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history_bn = model_bn.fit(train_images, train_labels, epochs=100, validation_split=0.2, \n",
    "                           callbacks=[early_stopping_cb, checkpoint_cb])\n",
    "```\n",
    "\n",
    "You can compare the `history` and `history_bn` variables to analyze the learning curves and check if\n",
    "Batch Normalization helps in faster convergence.\n",
    "\n",
    "### e. Add Dropout to every layer and evaluate:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# Build the model with Dropout\n",
    "model_dropout = Sequential()\n",
    "model_dropout.add(Dense(100, input_shape=(784,)))\n",
    "model_dropout.add(ELU())\n",
    "model_dropout.add(Dropout(0.5))\n",
    "model_dropout.add(Dense(100))\n",
    "model_dropout.add(ELU())\n",
    "model_dropout.add(Dropout(0.5))\n",
    "model_dropout.add(Dense(100))\n",
    "model_dropout.add(ELU())\n",
    "model_dropout.add(Dropout(0.5))\n",
    "model_dropout.add(Dense(100))\n",
    "model_dropout.add(ELU())\n",
    "model_dropout.add(Dropout(0.5))\n",
    "model_dropout.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile and train the model with Dropout\n",
    "model_dropout.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history_dropout = model_dropout.fit(train_images, train_labels, epochs=100, validation_split=0.2, \n",
    "                                   callbacks=[early_stopping_cb, checkpoint_cb])\n",
    "```\n",
    "\n",
    "You can analyze the `history_dropout` variable to see if adding Dropout to every layer helps in reducing overfitting.\n",
    "Adjust the dropout rate (0.5 in this example) as needed based on your observation of the model's performance on the\n",
    "validation set.\n",
    "\n",
    "Remember to evaluate the models on the test set to get a comprehensive understanding of their performance:\n",
    "\n",
    "```python\n",
    "# Evaluate the models on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "test_loss_bn, test_accuracy_bn = model_bn.evaluate(test_images, test_labels)\n",
    "test_loss_dropout, test_accuracy_dropout = model_dropout.evaluate(test_images, test_labels)\n",
    "\n",
    "print(\"DNN without Batch Normalization and Dropout - Test Accuracy:\", test_accuracy)\n",
    "print(\"DNN with Batch Normalization - Test Accuracy:\", test_accuracy_bn)\n",
    "print(\"DNN with Dropout - Test Accuracy:\", test_accuracy_dropout)\n",
    "```\n",
    "\n",
    "These steps will help you build, train, and evaluate the specified DNN architecture with different techniques\n",
    "like Batch Normalization and Dropout. Adjust hyperparameters and architectural choices as needed based on the \n",
    "observed performance during training and validation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. Transfer learning.\n",
    "a. Create a new DNN that reuses all the pretrained hidden layers of the previous\n",
    "model, freezes them, and replaces the softmax output layer with a new one.\n",
    "b. Train this new DNN on digits 5 to 9, using only 100 images per digit, and time how\n",
    "long it takes. Despite this small number of examples, can you achieve high precision?\n",
    "c. Try caching the frozen layers, and train the model again: how much faster is it now?\n",
    "d. Try again reusing just four hidden layers instead of five. Can you achieve a higher\n",
    "precision?\n",
    "e. Now unfreeze the top two hidden layers and continue training: can you get the\n",
    "model to perform even better?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "\n",
    "\n",
    "Certainly! Here's how you can approach the tasks:\n",
    "\n",
    "### a. Create a new DNN reusing pretrained hidden layers and replacing the output layer:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the pretrained DNN model\n",
    "pretrained_model = load_model(\"model.h5\")\n",
    "\n",
    "# Freeze all the pretrained hidden layers\n",
    "for layer in pretrained_model.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new output layer for digits 5 to 9\n",
    "new_output_layer = Dense(5, activation='softmax')\n",
    "\n",
    "# Build the new model\n",
    "transfer_model = Sequential([pretrained_model.layers[-2], new_output_layer])\n",
    "```\n",
    "\n",
    "### b. Train the new model on digits 5 to 9 with a small dataset:\n",
    "\n",
    "```python\n",
    "# Load MNIST data for digits 5 to 9\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_mask = (train_labels >= 5)\n",
    "test_mask = (test_labels >= 5)\n",
    "train_images, train_labels = train_images[train_mask], train_labels[train_mask]\n",
    "test_images, test_labels = test_images[test_mask], test_labels[test_mask]\n",
    "\n",
    "# Preprocess the data\n",
    "train_images = train_images.reshape((train_images.shape[0], -1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((test_images.shape[0], -1)).astype('float32') / 255\n",
    "\n",
    "# Compile and train the new model\n",
    "transfer_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history_transfer = transfer_model.fit(train_images, train_labels, epochs=50, validation_split=0.2)\n",
    "```\n",
    "\n",
    "### c. Cache frozen layers and train the model again:\n",
    "\n",
    "```python\n",
    "# Cache the frozen layers' activations\n",
    "for layer in pretrained_model.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Compile and train the model with cached frozen layers\n",
    "transfer_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history_transfer_cached = transfer_model.fit(train_images, train_labels, epochs=50, validation_split=0.2)\n",
    "```\n",
    "\n",
    "### d. Reuse four hidden layers and attempt to achieve higher precision:\n",
    "\n",
    "Modify the code in step `a` to reuse only four hidden layers and then proceed with training as before.\n",
    "\n",
    "```python\n",
    "# Load the pretrained DNN model\n",
    "pretrained_model = load_model(\"model.h5\")\n",
    "\n",
    "# Freeze only the first three hidden layers\n",
    "for layer in pretrained_model.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new output layer for digits 5 to 9\n",
    "new_output_layer = Dense(5, activation='softmax')\n",
    "\n",
    "# Build the new model\n",
    "transfer_model = Sequential([pretrained_model.layers[-4], pretrained_model.layers[-3],\n",
    "                             pretrained_model.layers[-2], pretrained_model.layers[-1], new_output_layer])\n",
    "\n",
    "# Compile and train the model with four hidden layers\n",
    "transfer_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history_transfer_four_layers = transfer_model.fit(train_images, train_labels, epochs=50, validation_split=0.2)\n",
    "```\n",
    "\n",
    "### e. Unfreeze top two hidden layers and continue training:\n",
    "\n",
    "```python\n",
    "# Unfreeze the top two hidden layers\n",
    "for layer in pretrained_model.layers[-4:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Compile and continue training the model with unfrozen layers\n",
    "transfer_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history_transfer_unfreeze = transfer_model.fit(train_images, train_labels, epochs=50, validation_split=0.2)\n",
    "```\n",
    "\n",
    "Remember to adjust the number of epochs, fine-tune hyperparameters, and monitor the model's performance using \n",
    "validation data to achieve the best results. Also, consider experimenting with different layers to unfreeze\n",
    "and observe the impact on the model's performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. Pretraining on an auxiliary task.\n",
    "a. In this exercise you will build a DNN that compares two MNIST digit images and\n",
    "predicts whether they represent the same digit or not. Then you will reuse the lower\n",
    "layers of this network to train an MNIST classifier using very little training data. Start\n",
    "by building two DNNs (let’s call them DNN A and B), both similar to the one you built\n",
    "earlier but without the output layer: each DNN should have five hidden layers of 100\n",
    "neurons each, He initialization, and ELU activation. Next, add one more hidden layer\n",
    "with 10 units on top of both DNNs. To do this, you should use\n",
    "TensorFlow’s concat() function with axis=1 to concatenate the outputs of both DNNs\n",
    "for each instance, then feed the result to the hidden layer. Finally, add an output\n",
    "layer with a single neuron using the logistic activation function.\n",
    "b. Split the MNIST training set in two sets: split #1 should containing 55,000 images,\n",
    "and split #2 should contain contain 5,000 images. Create a function that generates a\n",
    "training batch where each instance is a pair of MNIST images picked from split #1.\n",
    "Half of the training instances should be pairs of images that belong to the same\n",
    "class, while the other half should be images from different classes. For each pair, the\n",
    "\n",
    "training label should be 0 if the images are from the same class, or 1 if they are from\n",
    "different classes.\n",
    "c. Train the DNN on this training set. For each image pair, you can simultaneously feed\n",
    "the first image to DNN A and the second image to DNN B. The whole network will\n",
    "gradually learn to tell whether two images belong to the same class or not.\n",
    "d. Now create a new DNN by reusing and freezing the hidden layers of DNN A and\n",
    "adding a softmax output layer on top with 10 neurons. Train this network on split #2\n",
    "and see if you can achieve high performance despite having only 500 images per\n",
    "class.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ans-\n",
    "\n",
    "\n",
    "\n",
    "Certainly! Here's how you can approach this task:\n",
    "\n",
    "### a. Build DNNs for image comparison task:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, ELU, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Build DNN A\n",
    "input_A = tf.keras.layers.Input(shape=(784,))\n",
    "hidden_A = Dense(100, activation='elu', kernel_initializer='he_normal')(input_A)\n",
    "for _ in range(4):\n",
    "    hidden_A = Dense(100, activation='elu', kernel_initializer='he_normal')(hidden_A)\n",
    "output_A = Dense(10, activation='elu')(hidden_A)\n",
    "\n",
    "# Build DNN B\n",
    "input_B = tf.keras.layers.Input(shape=(784,))\n",
    "hidden_B = Dense(100, activation='elu', kernel_initializer='he_normal')(input_B)\n",
    "for _ in range(4):\n",
    "    hidden_B = Dense(100, activation='elu', kernel_initializer='he_normal')(hidden_B)\n",
    "output_B = Dense(10, activation='elu')(hidden_B)\n",
    "\n",
    "# Concatenate outputs of DNN A and DNN B\n",
    "concatenated_outputs = Concatenate(axis=1)([output_A, output_B])\n",
    "\n",
    "# Add hidden layer on top of concatenated outputs\n",
    "hidden_layer = Dense(10, activation='elu')(concatenated_outputs)\n",
    "\n",
    "# Output layer for binary classification\n",
    "output = Dense(1, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[input_A, input_B], outputs=output)\n",
    "```\n",
    "\n",
    "### b. Generate training batches:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Split the MNIST training set\n",
    "split_1_size = 55000\n",
    "split_2_size = 5000\n",
    "split_1_images, split_2_images = shuffle(train_images[:split_1_size], train_images[split_1_size:])\n",
    "split_1_labels, split_2_labels = shuffle(train_labels[:split_1_size], train_labels[split_1_size:])\n",
    "\n",
    "def generate_batch(images, labels, batch_size):\n",
    "    half_batch = batch_size // 2\n",
    "    while True:\n",
    "        positive_samples = np.random.choice(len(images), half_batch, replace=False)\n",
    "        negative_samples = np.random.choice(len(images), half_batch, replace=False)\n",
    "        \n",
    "        pairs = []\n",
    "        targets = []\n",
    "        for i in range(half_batch):\n",
    "            idx1, idx2 = positive_samples[i], negative_samples[i]\n",
    "            pairs += [[images[idx1], images[idx2]]]\n",
    "            targets += [0]\n",
    "            \n",
    "            idx1, idx2 = positive_samples[i], positive_samples[i]\n",
    "            pairs += [[images[idx1], images[idx2]]]\n",
    "            targets += [1]\n",
    "        \n",
    "        pairs, targets = shuffle(pairs, targets)\n",
    "        yield [np.array(pairs)[:,0], np.array(pairs)[:,1]], np.array(targets)\n",
    "        \n",
    "batch_size = 32\n",
    "train_gen = generate_batch(split_1_images, split_1_labels, batch_size)\n",
    "```\n",
    "\n",
    "### c. Train the DNN on the training set:\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_gen, epochs=10, steps_per_epoch=len(split_1_images) // batch_size)\n",
    "```\n",
    "\n",
    "### d. Create and train a new DNN for MNIST classification using split #2:\n",
    "\n",
    "```python\n",
    "# Reuse and freeze hidden layers of DNN A\n",
    "for layer in model.layers[:-2]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add softmax output layer for 10 classes\n",
    "output_layer = Dense(10, activation='softmax')(model.layers[-2].output)\n",
    "mnist_classifier = Model(inputs=model.inputs, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "mnist_classifier.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train on split #2\n",
    "mnist_classifier.fit([split_2_images, split_2_images], split_2_labels, epochs=10, batch_size=32, validation_split=0.2)\n",
    "```\n",
    "\n",
    "This approach uses the concept of siamese networks for image comparison and transfer learning to achieve good\n",
    "performance on a limited dataset. Fine-tuning the pretrained layers on a smaller dataset can lead to efficient \n",
    "utilization of data and model generalization. Remember to adjust hyperparameters and network architecture based\n",
    "on the performance on validation data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
